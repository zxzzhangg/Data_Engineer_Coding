,titles,descriptions,authors,urls
0,Computing the 2-adic complexity of two classes of Ding-Helleseth generalized cyclotomic sequences of period of twin prime products.,This paper contributes to compute 2-adic complexity of two classes ofDing-Helleseth generalized cyclotomic sequences. Results show that 2-adiccomplexity of these sequences is good enough to resist the attack by therational approximation algorithm.," Ming Yan, Tongjiang Yan, Yu Li",http://arxiv.org/abs/1912.06134
1,L3DOR: Lifelong 3D Object Recognition.,"3D object recognition has been widely-applied. However, moststate-of-the-arts are facing with a fixed recognition task set, which cannotwell tackle the new coming data with incremental tasks as human ourselves.Meanwhile, the performance of most state-of-the-art lifelong learning modelscan be deteriorated easily on previously learned recognition tasks, due to theexisting of unordered, large-scale, and irregular 3D geometry data. To addressthis challenge, in this paper, we propose a Lifelong 3D Object Recognition(i.e., L3DOR framework, which can consecutively learn new 3D object recognitiontasks via imitating ""human learning"". Specifically, the core idea of ourproposed L3DOR is to factorize PointNet in a perspective of lifelong learning,while capturing and storing the shared point-knowledge in a perspective oflayer-wise tensor factorization architecture. To further transfer thetask-specific knowledge from previous tasks to the new coming recognition task,a memory attention mechanism is proposed to connect the current task withrelevant previously tasks, which can effectively prevent catastrophicforgetting via soft-transferring previous knowledge. To our best knowledge,this is the first work about using lifelong learning to handle 3D objectrecognition task without model fine-tuning or retraining. Further, our L3DORcan also be extended to other backbone network (e.g., PointNet++). To the end,comparisons on several point cloud datasets validate that our L3DOR model canreduce averaged 1.68~3.36 times parameters for the overall model, withoutsacrificing recognition accuracy of each task."," Yuyang Liu, Yang Cong, Gan Sun",http://arxiv.org/abs/1912.06135
2,Calibrated model-based evidential clustering using bootstrapping.,"Evidential clustering is an approach to clustering in whichcluster-membership uncertainty is represented by a collection ofDempster-Shafer mass functions forming an evidential partition. In this paper,we propose to construct these mass functions by bootstrapping finite mixturemodels. In the first step, we compute bootstrap percentile confidence intervalsfor all pairwise probabilities (the probabilities for any two objects to belongto the same class). We then construct an evidential partition such that thepairwise belief and plausibility degrees approximate the bounds of theconfidence intervals. This evidential partition is calibrated, in the sensethat the pairwise belief-plausibility intervals contain the true probabilities""most of the time"", i.e., with a probability close to the defined confidencelevel. This frequentist property is verified by simulation, and the practicalapplicability of the method is demonstrated using several real datasets.", Thierry Denoeux,http://arxiv.org/abs/1912.06137
3,ABOUT ML: Annotation and Benchmarking on Understanding and Transparency of Machine Learning Lifecycles.,"We present the ""Annotation and Benchmarking on Understanding and Transparencyof Machine Learning Lifecycles"" (ABOUT ML) project as an initiative tooperationalize ML transparency and work towards a standard ML documentationpractice. We make the case for the project&apos;s relevance and effectiveness inconsolidating disparate efforts across a variety of stakeholders, as well asbringing in the perspectives of currently missing voices that will be valuablein shaping future conversations. We describe the details of the initiative andthe gaps we hope this project will help address."," Inioluwa Deborah Raji, Jingying Yang",http://arxiv.org/abs/1912.06166
4,Awareness in Practice: Tensions in Access to Sensitive Attribute Data for Antidiscrimination.,"Organizations cannot address demographic disparities that they cannot see.Recent research on machine learning and fairness has emphasized that awarenessof sensitive attributes, such as race and sex, is critical to the developmentof interventions. However, on the ground, the existence of these data cannot betaken for granted.</p><p>This paper uses the domains of employment, credit, and healthcare in theUnited States to surface conditions that have shaped the availability ofsensitive attribute data. For each domain, we describe how and when privatecompanies collect or infer sensitive attribute data for antidiscriminationpurposes. An inconsistent story emerges: Some companies are required by law tocollect sensitive attribute data, while others are prohibited from doing so.Still others, in the absence of legal mandates, have determined that collectionand imputation of these data are appropriate to address disparities.</p><p>This story has important implications for fairness research and its futureapplications. If companies that mediate access to life opportunities are unableor hesitant to collect or infer sensitive attribute data, then proposedtechniques to detect and mitigate bias in machine learning models might neverbe implemented outside the lab. We conclude that today&apos;s legal requirements andcorporate practices, while highly inconsistent across domains, offer lessonsfor how to approach the collection and inference of sensitive data inappropriate circumstances. We urge stakeholders, including machine learningpractitioners, to actively help chart a path forward that takes both policygoals and technical needs into account."," Miranda Bogen, Aaron Rieke, Shazeda Ahmed",http://arxiv.org/abs/1912.06171
5,Coevolution of Generative Adversarial Networks.,"Generative adversarial networks (GAN) became a hot topic, presentingimpressive results in the field of computer vision. However, there are stillopen problems with the GAN model, such as the training stability and thehand-design of architectures. Neuroevolution is a technique that can be used toprovide the automatic design of network architectures even in large searchspaces as in deep neural networks. Therefore, this project proposes COEGAN, amodel that combines neuroevolution and coevolution in the coordination of theGAN training algorithm. The proposal uses the adversarial characteristicbetween the generator and discriminator components to design an algorithm usingcoevolution techniques. Our proposal was evaluated in the MNIST dataset. Theresults suggest the improvement of the training stability and the automaticdiscovery of efficient network architectures for GANs. Our model also partiallysolves the mode collapse problem."," Victor Costa, Nuno Louren&#xe7;o, Penousal Machado",http://arxiv.org/abs/1912.06172
6,Training without training data: Improving the generalizability of automated medical abbreviation disambiguation.,"Abbreviation disambiguation is important for automated clinical noteprocessing due to the frequent use of abbreviations in clinical settings.Current models for automated abbreviation disambiguation are restricted by thescarcity and imbalance of labeled training data, decreasing theirgeneralizability to orthogonal sources. In this work we propose a novel dataaugmentation technique that utilizes information from related medical concepts,which improves our model&apos;s ability to generalize. Furthermore, we show thatincorporating the global context information within the whole medical note (inaddition to the traditional local context window), can significantly improvethe model&apos;s representation for abbreviations. We train our model on a publicdataset (MIMIC III) and test its performance on datasets from different sources(CASI, i2b2). Together, these two techniques boost the accuracy of abbreviationdisambiguation by almost 14% on the CASI dataset and 4% on i2b2."," Marta Skreta, Aryan Arbabi, Jixuan Wang, Michael Brudno",http://arxiv.org/abs/1912.06174
7,Investigating the effectiveness of web adblockers.,"We investigate adblocking filters and the extent to which websites andadvertisers react when their content is impacted by these filters. We collecteddata daily from the Alexa Top-5000 web sites for 120 days, and from specificsites that newly appeared in filter lists for 140 days. By evaluating how longa filter rule triggers on a website, we can gauge how long it remainseffective. We matched websites with both a regular adblocking filter list(EasyList) and with a specialized filter list that targets anti-adblockinglogic (Nano Defender).</p><p>From our data, we observe that the effectiveness of the EasyList adblockingfilter decays a modest 0.13\% per day, and after around 80 days seems tostabilize. We found no evidence for any significant decay in effectiveness ofthe more specialized, but less widely used, anti-adblocking removal filters."," Clayton Drazner, Nikola &#x110;uza, Hugo Jonker, Dan S. Wallach",http://arxiv.org/abs/1912.06176
8,COEGAN: Evaluating the Coevolution Effect in Generative Adversarial Networks.,"Generative adversarial networks (GAN) present state-of-the-art results in thegeneration of samples following the distribution of the input dataset. However,GANs are difficult to train, and several aspects of the model should bepreviously designed by hand. Neuroevolution is a well-known technique used toprovide the automatic design of network architectures which was recentlyexpanded to deep neural networks.</p><p>COEGAN is a model that uses neuroevolution and coevolution in the GANtraining algorithm to provide a more stable training method and the automaticdesign of neural network architectures. COEGAN makes use of the adversarialaspect of the GAN components to implement coevolutionary strategies in thetraining algorithm. Our proposal was evaluated in the Fashion-MNIST and MNISTdataset. We compare our results with a baseline based on DCGAN and also withresults from a random search algorithm. We show that our method is able todiscover efficient architectures in the Fashion-MNIST and MNIST datasets. Theresults also suggest that COEGAN can be used as a training algorithm for GANsto avoid common issues, such as the mode collapse problem."," Victor Costa, Nuno Louren&#xe7;o, Jo&#xe3;o Correia, Penousal Machado",http://arxiv.org/abs/1912.06180
9,Learning Effective Visual Relationship Detector on 1 GPU.,"We present our winning solution to the Open Images 2019 Visual Relationshipchallenge. This is the largest challenge of its kind to date with nearly 9million training images. Challenge task consists of detecting objects andidentifying relationships between them in complex scenes. Our solution hasthree stages, first object detection model is fine-tuned for the challengeclasses using a novel weight transfer approach. Then, spatio-semantic andvisual relationship models are trained on candidate object pairs. Finally,features and model predictions are combined to generate the final relationshipprediction. Throughout the challenge we focused on minimizing the hardwarerequirements of our architecture. Specifically, our weight transfer approachenables much faster optimization, allowing the entire architecture to betrained on a single GPU in under two days. In addition to efficientoptimization, our approach also achieves superior accuracy winning first placeout of over 200 teams, and outperforming the second place team by over $5\%$ onthe held-out private leaderboard."," Yichao Lu, Cheng Chang, Himanshu Rai, Guangwei Yu, Maksims Volkovs",http://arxiv.org/abs/1912.06185
10,Double descent in the condition number.,"In solving a system of $n$ linear equations in $d$ variables $Ax=b$, thecondition number of the $n,d$ matrix $A$ measures how much errors in the data$b$ affect the solution $x$. Bounds of this type are important in many inverseproblems. An example is machine learning where the key task is to estimate anunderlying function from a set of measurements at random points in a highdimensional space and where low sensitivity to error in the data is arequirement for good predictive performance. Here we discuss the simpleobservation, which is well-known but surprisingly little quoted that when thecolumns of $A$ are random vectors, the condition number of $A$ is highest if$d=n$, that is when the inverse of $A$ exists. An overdetermined system ($n&gt;d$)as well as an underdetermined system ($n&lt;d$), for which the pseudoinverse mustbe used instead of the inverse, typically have significantly better, that islower, condition numbers. Thus the condition number of $A$ plotted as functionof $d$ shows a double descent behavior with a peak at $d=n$."," Tomaso Poggio, Gil Kur, Andrzej Banburski",http://arxiv.org/abs/1912.06190
11,idris-ct: A Library to do Category Theory in Idris.,"We introduce idris-ct, a Idris library providing verified type definitions ofcategorical concepts. idris-ct strives to be a bridge between academy andindustry, catering both to category theorists who want to implement and trytheir ideas in a practical environment and to businesses and engineers who careabout formalization with category theory: It is inspired by similar librariesdeveloped for theorem proving but remains very practical, being aimed atsoftware production in business. Nevertheless, the use of dependent typesallows for a formally correct implementation of categorical concepts, so thatguarantees can be made on software properties."," Fabrizio Genovese, Alex Gryzlov, Jelle Herold, Andre Knispel, Marco Perone, Erik Post, Andr&#xe9; Videla",http://arxiv.org/abs/1912.06191
12,A new method for similarity and anomaly detection in cryptocurrency markets.,"We propose a new approach using the MJ$_1$ semi-metric, from the more generalMJ$_p$ class of semi-metrics \cite{James2019}, to detect similarity andanomalies in collections of cryptocurrencies. Since change points are signalsof potential risk, we apply this metric to measure distance between changepoint sets, with respect to returns and variance. Such change point sets can beidentified using algorithms such as the Mann-Whitney test, while the distancematrix is analysed using three approaches to detect similarity and identifyclusters of similar cryptocurrencies. This aims to avoid constructingportfolios with highly similar behaviours, reducing total portfolio risk."," Nick James, Max Menzies, Jennifer Chan",http://arxiv.org/abs/1912.06193
13,Data Exploration and Validation on dense knowledge graphs for biomedical research.,"Here we present a holistic approach for data exploration on dense knowledgegraphs as a novel approach with a proof-of-concept in biomedical research.Knowledge graphs are increasingly becoming a vital factor in knowledge miningand discovery as they connect data using technologies from the semantic web. Inthis paper we extend a basic knowledge graph extracted from biomedicalliterature by context data like named entities and relations obtained by textmining and other linked data sources like ontologies and databases. We willpresent an overview about this novel network. The aim of this work was toextend this current knowledge with approaches from graph theory. This methodwill build the foundation for quality control, validation of hypothesis,detection of missing data and time series analysis of biomedical knowledge ingeneral. In this context we tried to apply multiple-valued decision diagrams tothese questions. In addition this knowledge representation of linked data canbe used as FAIR approach to answer semantic questions. This paper sheds newlights on dense and very large knowledge graphs and the importance of agraph-theoretic understanding of these networks."," Jens D&#xf6;rpinghaus, Alexander Apke, Vanessa Lage-Rupprecht, Andreas Stefan",http://arxiv.org/abs/1912.06194
14,A mixture theory-based finite element formulation for the study of biodegradation of poroelastic scaffolds.,"We derive a mixture theory-based mathematical model of the degradation of aporoelastic solid immersed in a fluid bath. The evolution of the solid&apos;smechanical and transport properties are also modeled. The inspiration for themodel is the study of the temporal evolution of biodegradable Tissue EngineeredNerve Guides (TENGs), which are surgical implants supporting the alignment andre-growth of damaged nerves. The model comprises of the degrading solid, thedegradation reaction products, and the fluid in which the solid is immersed.The weak formulation of the partial differential equations (PDEs) so derived isnumerically implemented using a Finite Element Method (FEM). The numericalmodel is studied for stability and convergence rates using the Method ofManufactured Solutions."," Priyanka Patki, Francesco Costanzo",http://arxiv.org/abs/1912.06195
15,CRNs Exposed: Systematic Exploration of Chemical Reaction Networks.,"Formal methods have enabled breakthroughs in many fields, such as in hardwareverification, machine learning and biological systems. Our focus is on systemsand synthetic biology, where a key object of interest is coupled chemicalreactions in a well-mixed solution formalized as chemical reaction networks(CRNs). CRNs are pivotal for our understanding of biological regulatory andmetabolic networks, as well as for programming engineered molecular behavior.Although it is clear that small CRNs are capable of complex dynamics andcomputational behavior, it remains difficult to explore the space of CRNs insearch for desired functionality. We use Alloy, a tool for expressingstructural constraints and behavior in software systems, to enumerate CRNs withdeclaratively specified properties. We show how this framework can enumerateCRNs with a variety of structural constraints including biologically motivatedcatalytic networks and metabolic networks, and see-saw networks motivated byDNA nanotechnology. We also use the framework to explore analog functioncomputation in rate-independent CRNs. By computing the desired output valuewith stoichiometry rather than with reaction rates (in the sense that $X \toY+Y$ computes multiplication by $2$), such CRNs are completely robust to thechoice of reaction rates or rate law. We find the smallest CRNs computing themax, abs, and ReLU (rectified linear unit) functions in a natural subclass ofrate-independent CRNs where rate-independence follows from structural networkproperties."," Marko Vasic, David Soloveichik, Sarfraz Khurshid",http://arxiv.org/abs/1912.06197
16,A Constant-Factor Approximation for Directed Latency in Quasi-Polynomial Time.,"We give the first constant-factor approximation for the Directed Latencyproblem in quasi-polynomial time. Here, the goal is to visit all nodes in anasymmetric metric with a single vehicle starting at a depot $r$ to minimize theaverage time a node waits to be visited by the vehicle. The approximationguarantee is an improvement over the polynomial-time $O(\log n)$-approximation[Friggstad, Salavatipour, Svitkina, 2013] and no better quasi-polynomial timeapproximation algorithm was known.</p><p>To obtain this, we must extend a recent result showing the integrality gap ofthe Asymmetric TSP-Path LP relaxation is bounded by a constant [K\""{o}hne,Traub, and Vygen, 2019], which itself builds on the breakthrough result thatthe integrality gap for standard Asymmetric TSP is also a constant [Svensson,Tarnawsi, and Vegh, 2018]. We show the standard Asymmetric TSP-Path integralitygap is bounded by a constant even if the cut requirements of the LP relaxationare relaxed from $x(\delta^{in}(S)) \geq 1$ to $x(\delta^{in}(S)) \geq \rho$for some constant $1/2 &lt; \rho \leq 1$. We also give a better approximationguarantee in the special case of Directed Latency in regret metrics where thegoal is to find a path $P$ minimize the average time a node $v$ waits in excessof $c_{rv}$, i.e. $\frac{1}{|V|} \cdot \sum_{v \in V} (c_v(P)-c_{rv})$."," Zachary Friggstad, Chaitanya Swamy",http://arxiv.org/abs/1912.06198
17,Greenery Segmentation In Urban Images By Deep Learning.,Vegetation is a relevant feature in the urban scenery and its awareness canbe measured in an image by the Green View Index (GVI). Previous approaches toestimate the GVI were based upon heuristics image processing approaches andrecently by deep learning networks (DLN). By leveraging some recent DLNarchitectures tuned to the image segmentation problem and exploiting aweighting strategy in the loss function (LF) we improved previously reportedresults in similar datasets.," Artur A. M. Oliveira, Nina S. T. Hirata, Roberto Hirata Jr",http://arxiv.org/abs/1912.06199
18,On Metrics to Assess the Transferability of Machine Learning Models in Non-Intrusive Load Monitoring.,"To assess the performance of load disaggregation algorithms it is commonpractise to train a candidate algorithm on data from one or multiple householdsand subsequently apply cross-validation by evaluating the classification andenergy estimation performance on unseen portions of the dataset derived fromthe same households. With an emerging discussion of transferability inNon-Intrusive Load Monitoring (NILM), there is a need for domain-specificmetrics to assess the performance of NILM algorithms on new test scenariosbeing unseen buildings. In this paper, we discuss several metrics to assess thegeneralisation ability of NILM algorithms. These metrics target differentaspects of performance evaluation in NILM and are meant to complement thetraditional performance evaluation approach. We demonstrate how our metrics canbe utilised to evaluate NILM algorithms by means of two case studies. Weconduct our studies on several energy consumption datasets and take intoconsideration five state-of-the-art as well as four baseline NILM solutions.Finally, we formulate research challenges for future work."," Christoph Klemenjak, Anthony Faustine, Stephen Makonin, Wilfried Elmenreich",http://arxiv.org/abs/1912.06200
19,"Optimal, Truthful, and Private Securities Lending.","We consider a fundamental dynamic allocation problem motivated by the problemof $\textit{securities lending}$ in financial markets, the mechanism underlyingthe short selling of stocks. A lender would like to distribute a finite numberof identical copies of some scarce resource to $n$ clients, each of whom has aprivate demand that is unknown to the lender. The lender would like to maximizethe usage of the resource $\mbox{---}$ avoiding allocating more to a clientthan her true demand $\mbox{---}$ but is constrained to sell the resource at apre-specified price per unit, and thus cannot use prices to incentivizetruthful reporting. We first show that the Bayesian optimal algorithm for theone-shot problem $\mbox{---}$ which maximizes the resource&apos;s expected usageaccording to the posterior expectation of demand, given reports $\mbox{---}$actually incentivizes truthful reporting as a dominant strategy. Because truedemands in the securities lending problem are often sensitive information thatthe client would like to hide from competitors, we then consider the problemunder the additional desideratum of (joint) differential privacy. We give analgorithm, based on simple dynamics for computing market equilibria, that issimultaneously private, approximately optimal, and approximatelydominant-strategy truthful. Finally, we leverage this private algorithm toconstruct an approximately truthful, optimal mechanism for the extensive formmulti-round auction where the lender does not have access to the true jointdistributions between clients&apos; requests and demands."," Emily Diana, Michael Kearns, Seth Neel, Aaron Roth",http://arxiv.org/abs/1912.06202
20,ManiGAN: Text-Guided Image Manipulation.,"The goal of our paper is to semantically edit parts of an image to match agiven text that describes desired attributes (e.g., texture, colour, andbackground), while preserving other contents that are irrelevant to the text.To achieve this, we propose a novel generative adversarial network (ManiGAN),which contains two key components: text-image affine combination module (ACM)and detail correction module (DCM). The ACM selects image regions relevant tothe given text and then correlates the regions with corresponding semanticwords for effective manipulation. Meanwhile, it encodes original image featuresto help reconstruct text-irrelevant contents. The DCM rectifies mismatchedattributes and completes missing contents of the synthetic image. Finally, wesuggest a new metric for evaluating image manipulation results, in terms ofboth the generation of new attributes and the reconstruction of text-irrelevantcontents. Extensive experiments on the CUB and COCO datasets demonstrate thesuperior performance of the proposed method."," Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, Philip H. S. Torr",http://arxiv.org/abs/1912.06203
21,From deep learning to mechanistic understanding in neuroscience: the structure of retinal prediction.,"Recently, deep feedforward neural networks have achieved considerable successin modeling biological sensory processing, in terms of reproducing theinput-output map of sensory neurons. However, such models raise profoundquestions about the very nature of explanation in neuroscience. Are we simplyreplacing one complex system (a biological circuit) with another (a deepnetwork), without understanding either? Moreover, beyond neuralrepresentations, are the deep network&apos;s computational mechanisms for generatingneural responses the same as those in the brain? Without a systematic approachto extracting and understanding computational mechanisms from deep neuralnetwork models, it can be difficult both to assess the degree of utility ofdeep learning approaches in neuroscience, and to extract experimentallytestable hypotheses from deep networks. We develop such a systematic approachby combining dimensionality reduction and modern attribution methods fordetermining the relative importance of interneurons for specific visualcomputations. We apply this approach to deep network models of the retina,revealing a conceptual understanding of how the retina acts as a predictivefeature extractor that signals deviations from expectations for diversespatiotemporal stimuli. For each stimulus, our extracted computationalmechanisms are consistent with prior scientific literature, and in one caseyields a new mechanistic hypothesis. Thus overall, this work not only yieldsinsights into the computational mechanisms underlying the striking predictivecapabilities of the retina, but also places the framework of deep networks asneuroscientific models on firmer theoretical foundations, by providing a newroadmap to go beyond comparing neural representations to extracting andunderstand computational mechanisms."," Hidenori Tanaka, Aran Nayebi, Niru Maheswaranathan, Lane McIntosh, Stephen A. Baccus, Surya Ganguli",http://arxiv.org/abs/1912.06207
22,Shaping representations through communication: community size effect in artificial learning systems.,"Motivated by theories of language and communication that explain whycommunities with large numbers of speakers have, on average, simpler languageswith more regularity, we cast the representation learning problem in terms oflearning to communicate. Our starting point sees the traditional autoencodersetup as a single encoder with a fixed decoder partner that must learn tocommunicate. Generalizing from there, we introduce community-based autoencodersin which multiple encoders and decoders collectively learn representations bybeing randomly paired up on successive training iterations. We find thatincreasing community sizes reduce idiosyncrasies in the learned codes,resulting in representations that better encode concept categories andcorrelate with human feature norms."," Olivier Tieleman, Angeliki Lazaridou, Shibl Mourad, Charles Blundell, Doina Precup",http://arxiv.org/abs/1912.06208
23,Context-aware Entity Linking with Attentive Neural Networks on Wikidata Knowledge Graph.,"The Entity Linking (EL) approaches have been a long-standing research fieldand find applicability in various use cases such as semantic search, textannotation, question answering, etc. Although effective and robust, currentapproaches are still limited to particular knowledge repositories (e.g.Wikipedia) or specific knowledge graphs (e.g. Freebase, DBpedia, and YAGO). Thecollaborative knowledge graphs such as Wikidata excessively rely on the crowdto author the information. Since the crowd is not bound to a standard protocolfor assigning entity titles, the knowledge graph is populated by non-standard,noisy, long or even sometimes awkward titles. The issue of long, implicit, andnonstandard entity representations is a challenge in EL approaches for gaininghigh precision and recall. In this paper, we advance the state-of-the-artapproaches by developing a context-aware attentive neural network approach forentity linking on Wikidata. Our approach contributes by exploiting thesufficient context from a Knowledge Graph as a source of background knowledge,which is then fed into the neural network. This approach demonstrates merit toaddress challenges associated with entity titles (multi-word, long, implicit,case-sensitive). Our experimental study shows $\approx$8\% improvements overthe baseline approach, and significantly outperform an end to end approach forWikidata entity linking. This work, first of its kind, opens a new directionfor the research community to pay attention to developing context-aware ELapproaches for collaborative knowledge graphs."," Isaiah Onando Mulang, Kuldeep Singh, Akhilesh Vyas, Saeedeh Shekarpour, Ahmad Sakor, Maria Esther Vidal, Soren Auer, Jens Lehmann",http://arxiv.org/abs/1912.06214
24,Mixed-Precision analysis of Householder QR Algorithms.,"Although mixed precision arithmetic has recently garnered interest fortraining dense neural networks, many other applications could benefit from thespeed-ups and lower storage if applied appropriately. The growing interest inemploying mixed precision computations motivates the need for rounding erroranalysis that properly handles behavior from mixed precision arithmetic. Wepresent a framework for mixed precision analysis that builds on the foundationsof rounding error analysis presented in [Higham, Nicholas, ""Accuracy andStability of Numerical Algorithms"", SIAM] and demonstrate its practicality byapplying the analysis to various Householder QR Algorithms. In addition, wepresent successful results from using mixed precision QR factorization for somesmall-scale benchmark problems in graph clustering."," L. Minah Yang, Alyson Fox, Geoffrey Sanders",http://arxiv.org/abs/1912.06217
25,YOLACT++: Better Real-time Instance Segmentation.,"We present a simple, fully-convolutional model for real-time (&gt;30 fps)instance segmentation that achieves competitive results on MS COCO evaluated ona single Titan Xp, which is significantly faster than any previousstate-of-the-art approach. Moreover, we obtain this result after training ononly one GPU. We accomplish this by breaking instance segmentation into twoparallel subtasks: (1) generating a set of prototype masks and (2) predictingper-instance mask coefficients. Then we produce instance masks by linearlycombining the prototypes with the mask coefficients. We find that because thisprocess doesn&apos;t depend on repooling, this approach produces very high-qualitymasks and exhibits temporal stability for free. Furthermore, we analyze theemergent behavior of our prototypes and show they learn to localize instanceson their own in a translation variant manner, despite beingfully-convolutional. We also propose Fast NMS, a drop-in 12 ms fasterreplacement for standard NMS that only has a marginal performance penalty.Finally, by incorporating deformable convolutions into the backbone network,optimizing the prediction head with better anchor scales and aspect ratios, andadding a novel fast mask re-scoring branch, our YOLACT++ model can achieve 34.1mAP on MS COCO at 33.5 fps, which is fairly close to the state-of-the-artapproaches while still running at real-time."," Daniel Bolya, Chong Zhou, Fanyi Xiao, Yong Jae Lee",http://arxiv.org/abs/1912.06218
26,Theme-Matters: Fashion Compatibility Learning via Theme Attention.,"Fashion compatibility learning is important to many fashion markets such asoutfit composition and online fashion recommendation. Unlike previous work, weargue that fashion compatibility is not only a visual appearance compatibleproblem but also a theme-matters problem. An outfit, which consists of a set offashion items (e.g., shirt, suit, shoes, etc.), is considered to be compatiblefor a ""dating"" event, yet maybe not for a ""business"" occasion. In this paper,we aim at solving the fashion compatibility problem given specific themes. Tothis end, we built the first real-world theme-aware fashion dataset comprising14K around outfits labeled with 32 themes. In this dataset, there are more than40K fashion items labeled with 152 fine-grained categories. We also propose anattention model learning fashion compatibility given a specific theme. Itstarts with a category-specific subspace learning, which projects compatibleoutfit items in certain categories to be close in the subspace. Thanks tostrong connections between fashion themes and categories, we then build atheme-attention model over the category-specific embedding space. This modelassociates themes with the pairwise compatibility with attention, and thuscompute the outfit-wise compatibility. To the best of our knowledge, this isthe first attempt to estimate outfit compatibility conditional on a theme. Weconduct extensive qualitative and quantitative experiments on our new dataset.Our method outperforms the state-of-the-art approaches."," Jui-Hsin Lai, Bo Wu, Jingen Liu, Xin Wang, Dan Zeng, Tao Mei",http://arxiv.org/abs/1912.06227
27,Optimal Two-Sided Market Mechanism Design for Large-Scale Data Sharing and Trading in Massive IoT Networks.,"The development of the Internet of Things (IoT) generates a significantamount of data that contains valuable knowledge for system operations andbusiness opportunities. Since the data is the property of the IoT data owners,the access of the data requires the permission from the data owners, whichgives rises to a potential market opportunity for the IoT data sharing andtrading to create economic values and market opportunities for both data ownersand consumers. In this work, we leverage optimal mechanism design theory todevelop a monopolist matching platform for data trading over massive IoTnetworks. The proposed mechanism is composed of a pair of matching and paymentrules for each side of the market to optimize the payoffs underwelfare-maximization and the profit maximization schemes. We characterize theoptimal mechanism with a class of threshold matching rules under welfare andprofit-maximization and study three matching behaviors including separating,bottom pooling, and top pooling. We use HealthyGo as a case study to illustratethe optimal threshold matching rules and corroborate the analytical results."," Tao Zhang, Quanyan Zhu",http://arxiv.org/abs/1912.06229
28,The role of Web of Science publications in China&apos;s tenure system.,"Tenure provides a permanent position to faculty in higher educationinstitutions. In North America, it is granted to those who have established arecord of excellence in research, teaching and services in a limited period.However, in China, research excellence represented by the number of Web ofScience publications is highly weighted in the tenure assessment compared toexcellence in teaching and services, but this has never been systematicallyinvestigated. By analyzing the tenure assessment documents from Chineseuniversities, this study reveals the role of Web of Science publications inChina tenure system and presents the landscape of the tenure assessment processin Chinese higher education institutions."," Fei Shu, Wei Quan, Bikun Chen, Junping Qiu, Cassidy Sugimoto, Vincent Larivi&#xe8;re",http://arxiv.org/abs/1912.06231
29,Prior Knowledge Neural Network for Automatic Feature Construction in Financial Time Series.,"In quantitative finance, useful features are constructed by human experts.However, this method is of low efficient. Thus, automatic feature constructionalgorithms have received more and more attention. The SOTA technic in thisfield is to use reverse polish expression to represent the features, and thenuse genetic programming to reconstruct it. In this paper, we propose a newmethod, alpha discovery neural network, which can automatically constructfeatures by using neural network. In this work, we made several contributions.Firstly, we put forward new object function by using empirical knowledge infinancial signal processing, and we also fixed its undifferentiated problem.Secondly, we use model stealing technic to learn from other prior knowledge,which can bring enough diversity into our network. Thirdly, we come up with amethod to measure the diversity of different financial features. Experimentshows that ADN can produce more diversified and higher informative featuresthan GP. Besides, if we use GP&apos;s output to serve as prior knowledge, its finalachievements will be significantly improved by using ADN."," Jie Fang, Jianwu Lin, Yong Jiang, Shutao Xia",http://arxiv.org/abs/1912.06236
30,Fast non-convex low-rank matrix decomposition for separation of potential field data using minimal memory.,"A fast non-convex low-rank matrix decomposition method for potential fielddata separation is proposed. The singular value decomposition of the large sizetrajectory matrix, which is also a block Hankel matrix, is obtained using afast randomized singular value decomposition algorithm in which fast blockHankel matrix-vector multiplications are implemented with minimal memorystorage. This fast block Hankel matrix randomized singular value decompositionalgorithm is integrated into the \texttt{Altproj} algorithm, which is astandard non-convex method for solving the robust principal component analysisoptimization problem. The improved algorithm avoids the construction of thetrajectory matrix. Hence, gravity and magnetic data matrices of large size canbe computed. Moreover, it is more efficient than the traditional low-rankmatrix decomposition method, which is based on the use of an inexact augmentedLagrange multiplier algorithm. The presented algorithm is also robust and,hence, algorithm-dependent parameters are easily determined. The improved andtraditional algorithms are contrasted for the separation of synthetic gravityand magnetic data matrices of different sizes. The presented resultsdemonstrate that the improved algorithm is not only computationally moreefficient but it is also more accurate. Moreover, it is possible to solve farlarger problems. As an example, for the adopted computational environment,matrices of sizes larger than $205 \times 205$ generate ""out of memory""exceptions with the traditional method, but a matrix of size $2001\times 2001$can be calculated in $2249.69$s with the new algorithm. Finally, the improvedmethod is applied to separate real gravity and magnetic data in the Tonglingarea, Anhui province, China. Areas which may exhibit mineralizations areinferred based on the separated anomalies."," Dan Zhu, Rosemary Renaut, Hongwei Li, Tianyou Liu",http://arxiv.org/abs/1912.06240
31,Multi-Task Offloading over Vehicular Clouds under Graph-based Representation.,"Vehicular cloud computing has emerged as a promising paradigm for realizinguser requirements in computation-intensive tasks in modern drivingenvironments. In this paper, a novel framework of multi-task offloading overvehicular clouds (VCs) is introduced where tasks and VCs are modeled asundirected weighted graphs. Aiming to achieve a trade-off between minimizingtask completion time and data exchange costs, task components are efficientlymapped to available virtual machines in the related VCs. The problem isformulated as a non-linear integer programming problem, mainly underconstraints of limited contact between vehicles as well as available resources,and addressed in low-traffic and rush-hour scenarios. In low-traffic cases, wedetermine optimal solutions; in rush-hour cases, a connection-restrictedrandommatching-based subgraph isomorphism algorithm is proposed that presentslow computational complexity. Evaluations of the proposed algorithms againstgreedy-based baseline methods are conducted via extensive simulations."," Minghui Liwang, Zhibin Gao, Seyyedali Hosseinalipour, Huaiyu Dai",http://arxiv.org/abs/1912.06243
32,Screening of Informed and Uninformed Experts.,"Testing the validity of claims made by self-proclaimed experts can beimpossible when testing them in isolation, even with infinite observations atthe disposal of the tester. However, in a multiple expert setting it ispossible to design a contract that only informed experts accept and uninformedexperts reject. The tester can pit competing forecasts of future events againsteach other and take advantage of the uncertainty experts have about the otherexperts&apos; knowledge. This contract will work even when there is only a singledata point to evaluate."," Jorge Barreras (1 and 2), Alvaro Riascos (2 and 3) ((1) Department of Mathematics, University of Pennsylvania, (2) Quantil Research, Bogota, Colombia, (3) Department of Economics, Universidad de Los Andes)",http://arxiv.org/abs/1912.06244
33,General Information Bottleneck Objectives and their Applications to Machine Learning.,"We view the Information Bottleneck Principle (IBP: Tishby et al., 1999;Schwartz-Ziv and Tishby, 2017) and Predictive Information Bottleneck Principle(PIBP: Still et al., 2007; Alemi, 2019) as special cases of a family of generalinformation bottleneck objectives (IBOs). Each IBO corresponds to a particularconstrained optimization problem where the constraints apply to: (a) the mutualinformation between the training data and the learned model parameters orextracted representation of the data, and (b) the mutual information betweenthe learned model parameters or extracted representation of the data and thetest data (if any). The heuristics behind the IBP and PIBP are shown to yielddifferent constraints in the corresponding constrained optimization problemformulations. We show how other heuristics lead to a new IBO, different fromboth the IBP and PIBP, and use the techniques from (Alemi, 2019) to derive andoptimize a variational upper bound on the new IBO.</p><p>We then apply the theory of general IBOs to resolve the seeming contradictionbetween, on the one hand, the recommendations of IBP and PIBP to maximize themutual information between the model parameters and test data, and on theother, recent information-theoretic results (see Xu and Raginsky, 2017)suggesting that this mutual information should be minimized. The key insight isthat the heuristics (and thus the constraints in the constrained optimizationproblems) of IBP and PIBP are not applicable to the scenario analyzed by (Xuand Raginsky, 2017) because the latter makes the additional assumption that theparameters of the trained model have been selected to minimize the empiricalloss function. Aided by this insight, we formulate a new IBO that accounts forthis property of the parameters of the trained model, and derive and optimize avariational bound on this IBO.", Sayandev Mukherjee,http://arxiv.org/abs/1912.06248
34,A scaling-invariant algorithm for linear programming whose running time depends only on the constraint matrix.,"Following the breakthrough work of Tardos in the bit-complexity model,Vavasis and Ye gave the first exact algorithm for linear programming in thereal model of computation with running time depending only on the constraintmatrix. For solving a linear program (LP) $\max\, c^\top x,\: Ax = b,\: x \geq0,\: A \in \mathbb{R}^{m \times n}$, Vavasis and Ye developed a primal-dualinterior point method using a &apos;layered least squares&apos; (LLS) step, and showedthat $O(n^{3.5} \log (\bar{\chi}_A+n))$ iterations suffice to solve (LP)exactly, where $\bar{\chi}_A$ is a condition measure controlling the size ofsolutions to linear systems related to $A$.</p><p>Monteiro and Tsuchiya, noting that the central path is invariant underrescalings of the columns of $A$ and $c$, asked whether there exists an LPalgorithm depending instead on the measure $\bar{\chi}^*_A$, defined as theminimum $\bar{\chi}_{AD}$ value achievable by a column rescaling $AD$ of $A$,and gave strong evidence that this should be the case. We resolve this openquestion affirmatively.</p><p>Our first main contribution is an $O(m^2 n^2 + n^3)$ time algorithm whichworks on the linear matroid of $A$ to compute a nearly optimal diagonalrescaling $D$ satisfying $\bar{\chi}_{AD} \leq n(\bar{\chi}^*)^3$. Thisalgorithm also allows us to approximate the value of $\bar{\chi}_A$ up to afactor $n (\bar{\chi}^*)^2$. As our second main contribution, we develop ascaling invariant LLS algorithm, together with a refined potential functionbased analysis for LLS algorithms in general. With this analysis, we derive animproved $O(n^{2.5} \log n\log (\bar{\chi}^*_A+n))$ iteration bound foroptimally solving (LP) using our algorithm. The same argument also yields afactor $n/\log n$ improvement on the iteration complexity bound of the originalVavasis-Ye algorithm."," Daniel Dadush, Sophie Huiberts, Bento Natura, L&#xe1;szl&#xf3; A. V&#xe9;gh",http://arxiv.org/abs/1912.06252
35,Unconstrained Facial Expression Transfer using Style-based Generator.,"Facial expression transfer and reenactment has been an important researchproblem given its applications in face editing, image manipulation, andfabricated videos generation. We present a novel method for image-based facialexpression transfer, leveraging the recent style-based GAN shown to be veryeffective for creating realistic looking images. Given two face images, ourmethod can create plausible results that combine the appearance of one imageand the expression of the other. To achieve this, we first propose anoptimization procedure based on StyleGAN to infer hierarchical style vectorfrom an image that disentangle different attributes of the face. We furtherintroduce a linear combination scheme that fuses the style vectors of the twogiven images and generate a new face that combines the expression andappearance of the inputs. Our method can create high-quality synthesis withaccurate facial reenactment. Unlike many existing methods, we do not rely ongeometry annotations, and can be applied to unconstrained facial images of anyidentities without the need for retraining, making it feasible to generatelarge-scale expression-transferred results."," Chao Yang, Ser-Nam Lim",http://arxiv.org/abs/1912.06253
36,Theoretically-Efficient and Practical Parallel DBSCAN.,"The DBSCAN method for spatial clustering has received significant attentiondue to its applicability in a variety of data analysis tasks. There are fastsequential algorithms for DBSCAN in Euclidean space that take $O(n\log n)$ workfor two dimensions, sub-quadratic work for three or more dimensions, and can becomputed approximately in linear work for any constant number of dimensions.However, existing parallel DBSCAN algorithms require quadratic work in theworst case, making them inefficient for large datasets. This paper bridges thegap between theory and practice of parallel DBSCAN by presenting new parallelalgorithms for Euclidean exact DBSCAN and approximate DBSCAN that match thework bounds of their sequential counterparts, and are highly parallel(polylogarithmic depth). We present implementations of our algorithms alongwith optimizations that improve their practical performance. We perform acomprehensive experimental evaluation of our algorithms on a variety ofdatasets and parameter settings. Our experiments on a 36-core machine withhyper-threading show that we outperform existing parallel DBSCANimplementations by up to several orders of magnitude, and achieve speedups byup to 33x over the best sequential algorithms."," Yiqiu Wang, Yan Gu, Julian Shun",http://arxiv.org/abs/1912.06255
37,Mcity Data Collection for Automated Vehicles Study.,"The main goal of this paper is to introduce the data collection effort atMcity targeting automated vehicle development. We captured a comprehensive setof data from a set of perception sensors (Lidars, Radars, Cameras) as well asvehicle steering/brake/throttle inputs and an RTK unit. Two in-cabin camerasrecord the human driver&apos;s behaviors for possible future use. The naturalisticdriving on selected open roads is recorded at different time of day and weatherconditions. We also perform designed choreography data collection inside theMcity test facility focusing on vehicle to vehicle, and vehicle to vulnerableroad user interactions which is quite unique among existing open-sourcedatasets. The vehicle platform, data content, tags/labels, and selectedanalysis results are shown in this paper."," Yiqun Dong, Yuanxin Zhong, Wenbo Yu, Minghan Zhu, Pingping Lu, Yeyang Fang, Jiajun Hong, Huei Peng",http://arxiv.org/abs/1912.06258
38,A predictive path-following controller for multi-steered articulated vehicles.,"Stabilizing multi-steered articulated vehicles in backward motion is acomplex task for any human driver. Unless the vehicle is accurately steered,its structurally unstable joint-angle kinematics during reverse maneuvers cancause the vehicle segments to fold and enter a jack-knife state. In this work,a model predictive path-following controller is proposed enabling automaticlow-speed steering control of multi-steered articulated vehicles, comprising acar-like tractor and an arbitrary number of trailers with passive or activesteering. The proposed path-following controller is tailored to follow nominalpaths that contains full state and control-input information, and is designedto satisfy various physical constraints on the vehicle states as well assaturations and rate limitations on the tractor&apos;s curvature and the trailersteering angles. The performance of the proposed model predictivepath-following controller is evaluated in a set of simulations for amulti-steered 2-trailer with a car-like tractor where the last trailer hassteerable wheels."," Oskar Ljungqvist, Daniel Axehill",http://arxiv.org/abs/1912.06259
39,Extracting clinical concepts from user queries.,"Clinical concept extraction often begins with clinical Named EntityRecognition (NER). Often trained on annotated clinical notes, clinical NERmodels tend to struggle with tagging clinical entities in user queries becauseof the structural differences between clinical notes and user queries. Userqueries, unlike clinical notes, are often ungrammatical and incoherent. In manycases, user queries are compounded of multiple clinical entities, without commaor conjunction words separating them. By using as dataset a mixture ofannotated clinical notes and synthesized user queries, we adapt a clinical NERmodel based on the BiLSTM-CRF architecture for tagging clinical entities inuser queries. Our contribution are the following: 1) We found that when trainedon a mixture of synthesized user queries and clinical notes, the NER modelperforms better on both user queries and clinical notes. 2) We provide anend-to-end and easy-to-implement framework for clinical concept extraction fromuser queries."," Yue Zhao, John Handley",http://arxiv.org/abs/1912.06262
40,Optimization-based motion planning for multi-steered articulated vehicles.,"The task of maneuvering a multi-steered articulated vehicle in confinedenvironments is difficult even for experienced drivers. In this work, wepresent an optimization-based trajectory planner targeting low-speed maneuversin unstructured environments for multi-steered N-trailer vehicles, which arecomprised of a car-like tractor and an arbitrary number of interconnectedtrailers with fixed or steerable wheels. The proposed trajectory planningframework is divided into two steps, where a lattice-based trajectory planneris used in a first step to compute a resolution optimal solution to adiscretized version of the trajectory planning problem. The output from thelattice planner is then used in a second step to initialize an optimal controlproblem solver, which enables the framework to compute locally optimaltrajectories that start at the vehicle&apos;s initial state and reaches the goalstate exactly. The performance of the proposed optimization-based trajectoryplanner is evaluated in a set of practically relevant scenarios for amulti-steered 3-trailer vehicle with a car-like tractor where the last traileris steerable."," Oskar Ljungqvist, Kristoffer Bergman, Daniel Axehill",http://arxiv.org/abs/1912.06264
41,Towards Disentangled Representations for Human Retargeting by Multi-view Learning.,"We study the problem of learning disentangled representations for data acrossmultiple domains and its applications in human retargeting. Our goal is to mapan input image to an identity-invariant latent representation that capturesintrinsic factors such as expressions and poses. To this end, we present anovel multi-view learning approach that leverages various data sources such asimages, keypoints, and poses. Our model consists of multiple id-conditionedVAEs for different views of the data. During training, we encourage the latentembeddings to be consistent across these views. Our observation is thatauxiliary data like keypoints and poses contain critical, id-agnostic semanticinformation, and it is easier to train a disentangling CVAE on these simplerviews to separate such semantics from other id-specific attributes. We showthat training multi-view CVAEs and encourage latent-consistency guides theimage encoding to preserve the semantics of expressions and poses, leading toimproved disentangled representations and better human retargeting results."," Chao Yang, Xiaofeng Liu, Qingming Tang, C.-C. Jay Kuo",http://arxiv.org/abs/1912.06265
42,Inferring Distributions Over Depth from a Single Image.,"When building a geometric scene understanding system for autonomous vehicles,it is crucial to know when the system might fail. Most contemporary approachescast the problem as depth regression, whose output is a depth value for eachpixel. Such approaches cannot diagnose when failures might occur. Oneattractive alternative is a deep Bayesian network, which captures uncertaintyin both model parameters and ambiguous sensor measurements. However, estimatinguncertainties is often slow and the distributions are often limited to beuni-modal. In this paper, we recast the continuous problem of depth regressionas discrete binary classification, whose output is an un-normalizeddistribution over possible depths for each pixel. Such output allows one toreliably and efficiently capture multi-modal depth distributions in ambiguouscases, such as depth discontinuities and reflective surfaces. Results onstandard benchmarks show that our method produces accurate depth predictionsand significantly better uncertainty estimations than prior art while runningnear real-time. Finally, by making use of uncertainties of the predicteddistribution, we significantly reduce streak-like artifacts and improvesaccuracy as well as memory efficiency in 3D map reconstruction."," Gengshan Yang, Peiyun Hu, Deva Ramanan",http://arxiv.org/abs/1912.06268
43,Learning and Optimization with Bayesian Hybrid Models.,"Bayesian hybrid models fuse physics-based insights with machine learningconstructs to correct for systematic bias. In this paper, we compare Bayesianhybrid models against physics-based glass-box and Gaussian process black-boxsurrogate models. We consider ballistic firing as an illustrative case studyfor a Bayesian decision-making workflow. First, Bayesian calibration isperformed to estimate model parameters. We then use the posterior distributionfrom Bayesian analysis to compute optimal firing conditions to hit a target viaa single-stage stochastic program. The case study demonstrates the ability ofBayesian hybrid models to overcome systematic bias from missing physics withless data than the pure machine learning approach. Ultimately, we argueBayesian hybrid models are an emerging paradigm for data-informeddecision-making under parametric and epistemic uncertainty."," Elvis A. Eugene, Xian Gao, Alexander W. Dowling",http://arxiv.org/abs/1912.06269
44,An Asymptotically Compatible Formulation for Local-to-Nonlocal Coupling Problems without Overlapping Regions.,"In this paper we design and analyze an explicit partitioned procedure for a2D dynamic local-to-nonlocal (LtN) coupling problem, based on a new nonlocalRobin-type transmission condition. The nonlocal subproblem is modeled by thenonlocal heat equation with a finite horizon parameter $\delta$ characterizingthe range of nonlocal interactions, and the local subproblem is described bythe classical heat equation. We consider a heterogeneous system where the localand nonlocal subproblems present different physical properties, and employ nooverlapping region between the two subdomains. We first propose a newgeneralization of classical local Neumann-type condition by converting thelocal flux to a correction term in the nonlocal model, and show that theproposed Neumann-type boundary formulation recovers the local case as$O(\delta^2)$ in the $L^{\infty}$ norm. We then extend the nonlocalNeumann-type boundary condition to a Robin-type boundary condition, and developa local-to-nonlocal coupling formulation with Robin-Dirichlet transmissionconditions. To stabilize the explicit coupling procedure and to achieveasymptotic compatibility, the choice of the coefficient in the Robin conditionis obtained via amplification factor analysis for the discretized system withcoarse grids. Employing a high-order meshfree discretization method in thenonlocal solver and a linear finite element method in the local solver, theselection of optimal Robin coefficients are verified with numerical experimentson heterogeneous and complicated domains. With the developed optimal couplingstrategy, we numerically demonstrate the coupling framework&apos;s asymptoticconvergence to the local limit with an $O(\delta)=O(h)$ rate, when there is afixed ratio between the horizon size $\delta$ and the spatial discretizationsize $h$."," Huaiqian You, Yue Yu, David Kamensky",http://arxiv.org/abs/1912.06270
45,Federated learning with multichannel ALOHA.,"In this paper, we study federated learning in a cellular system with a basestation (BS) and a large number of users with local data sets. We show thatmultichannel random access can provide a better performance than sequentialpolling when some users are unable to compute local updates (due to othertasks) or in dormant state. In addition, for better aggregation in federatedlearning, the access probabilities of users can be optimized for given localupdates. To this end, we formulate an optimization problem and show that adistributed approach can be used within federated learning to adaptively decidethe access probabilities."," Jinho Choi, Shiva Raj Pokhrel",http://arxiv.org/abs/1912.06273
46,Joint Viewpoint and Keypoint Estimation with Real and Synthetic Data.,"The estimation of viewpoints and keypoints effectively enhance objectdetection methods by extracting valuable traits of the object instances. Whilethe output of both processes differ, i.e., angles vs. list of characteristicpoints, they indeed share the same focus on how the object is placed in thescene, inducing that there is a certain level of correlation between them.Therefore, we propose a convolutional neural network that jointly computes theviewpoint and keypoints for different object categories. By training both taskstogether, each task improves the accuracy of the other. Since the labelling ofobject keypoints is very time consuming for human annotators, we also introducea new synthetic dataset with automatically generated viewpoint and keypointsannotations. Our proposed network can also be trained on datasets that containviewpoint and keypoints annotations or only one of them. The experiments showthat the proposed approach successfully exploits this implicit correlationbetween the tasks and outperforms previous techniques that are trainedindependently."," Pau Panareda Busto, Juergen Gall",http://arxiv.org/abs/1912.06274
47,On Low-complexity Lattice Reduction Algorithms for Large-scale MIMO Detection: the Blessing of Sequential Reduction.,"Lattice reduction is a popular preprocessing strategy in multiple-inputmultiple-output (MIMO) detection. In a quest for developing a low-complexityreduction algorithm for large-scale problems, this paper investigates a newframework called sequential reduction (SR), which aims to reduce the lengths ofall basis vectors. The performance upper bounds of the strongest reduction inSR are given when the lattice dimension is no larger than 4. The proposed newframework enables the implementation of a hash-based low-complexity latticereduction algorithm, which becomes especially tempting when applied tolarge-scale MIMO detection. Simulation results show that, compared to otherreduction algorithms, the hash-based SR algorithm exhibits the lowestcomplexity while maintaining comparable error performance."," Shanxiang Lyu, Jinming Wen, Jian Weng, Cong Ling",http://arxiv.org/abs/1912.06278
48,Joint AGC and Receiver Design for Large-Scale MU-MIMO Systems with Coarsely Quantized Signals and C-RANs.,"In this work, we propose the joint optimization of the automatic gain control(AGC), which works in the remote radio heads (RHHs), and a low-resolution aware(LRA) linear receive filter based on the minimum mean square error (MMSE),which works on the baseband unit (BBU) pool, for large-scale multi-usermultiple-input multiple-output (MU-MIMO) systems with coarsely quantizedsignals in cloud radio access networks. We develop successive interferencecancellation receivers based on the proposed joint AGC and LRA-MMSE(AGC-LRA-MMSE) approach. An analysis of the achievable sum rates is alsocarried out. Simulations show that the proposed design has better error ratesand higher achievable rates than existing techniques."," T. Cunha, R. C. de Lamare, T. N. Ferreira",http://arxiv.org/abs/1912.06282
49,Mission Oriented Miniature Fixed-wing UAV Swarms: A Multi-layered and Distributed Architecture.,"UAV swarms have triggered wide concern due to their potential applicationvalues in recent years. While there are studies proposed in terms of thearchitecture design for UAV swarms, two main challenges still exist: (1)Scalability, supporting a large scale of vehicles; (2) Versatility, integratingdiversified missions. To this end, a multi-layered and distributed architecturefor mission oriented miniature fixed-wing UAV swarms is presented in thispaper. The proposed architecture is built on the concept of modularity. Itdivides the overall system to five layers: low-level control, high-levelcontrol, coordination, communication and human interaction layers, and manymodules that can be viewed as black boxes with interfaces of inputs andoutputs. In this way, not only the complexity of developing a large system canbe reduced, but also the versatility of supporting diversified missions can beensured. Furthermore, the proposed architecture is fully distributed that eachUAV performs the decision-making procedure autonomously so as to achieve betterscalability. Moreover, different kinds of aerial platforms can be feasiblyextended by using the control allocation matrices and the integrated hardwarebox. A prototype swarm system based on the proposed architecture is built andthe proposed architecture is evaluated through field experiments with a scaleof 21 fixed-wing UAVs. Particularly, to the best of our knowledge, this paperis the first work which successfully demonstrates formation flight, targetrecognition and tracking missions within an integrated architecture forfixed-wing UAV swarms through field experiments."," Zhihong Liu, Xiangke Wang, Lincheng Shen, Shulong Zhao, Yirui Cong, Jie Li, Dong Yin, Shengde Jia, Xiaojia Xiang",http://arxiv.org/abs/1912.06285
50,Meta-Learning Initializations for Image Segmentation.,"While meta-learning approaches that utilize neural network representationshave made progress in few-shot image classification, reinforcement learning,and, more recently, image semantic segmentation, the training algorithms andmodel architectures have become increasingly specialized to the few-shotdomain. A natural question that arises is how to develop learning systems thatscale from few-shot to many-shot settings while yielding competitiveperformance in both. One scalable potential approach that does not requireensembling many models nor the computational costs of relation networks, is tometa-learn an initialization. In this work, we study first-order meta-learningof initializations for deep neural networks that must produce dense, structuredpredictions given an arbitrary amount of training data for a new task. Ourprimary contributions include (1), an extension and experimental analysis offirst-order model agnostic meta-learning algorithms (including FOMAML andReptile) to image segmentation, (2) a novel neural network architecture builtfor parameter efficiency and fast learning which we call EfficientLab, (3) aformalization of the generalization error of meta-learning algorithms, which weleverage to decrease error on unseen tasks, and (4) a small benchmark dataset,FP-k, for the empirical study of how meta-learning systems perform in both few-and many-shot settings. We show that meta-learned initializations for imagesegmentation provide value for both canonical few-shot learning problems andlarger datasets, outperforming ImageNet-trained initializations for up to 400densely labeled examples. We find that our network, with an empiricallyestimated optimal update procedure, yields state of the art results on theFSS-1000 dataset while only requiring one forward pass through a single modelat evaluation time."," Sean M. Hendryx, Andrew B. Leach, Paul D. Hein, Clayton T. Morrison",http://arxiv.org/abs/1912.06290
51,More Efficient Off-Policy Evaluation through Regularized Targeted Learning.,"We study the problem of off-policy evaluation (OPE) in Reinforcement Learning(RL), where the aim is to estimate the performance of a new policy givenhistorical data that may have been generated by a different policy, orpolicies. In particular, we introduce a novel doubly-robust estimator for theOPE problem in RL, based on the Targeted Maximum Likelihood Estimationprinciple from the statistical causal inference literature. We also introduceseveral variance reduction techniques that lead to impressive performance gainsin off-policy evaluation. We show empirically that our estimator uniformly winsover existing off-policy evaluation methods across multiple RL environments andvarious levels of model misspecification. Finally, we further the existingtheoretical analysis of estimators for the RL off-policy estimation problem byshowing their $O_P(1/\sqrt{n})$ rate of convergence and characterizing theirasymptotic distribution."," Aur&#xe9;lien F. Bibaut, Ivana Malenica, Nikos Vlassis, Mark J. van der Laan",http://arxiv.org/abs/1912.06292
52,A Practical Solution for SAR Despeckling with Only Single Speckled Images.,"In this letter, we aim to address synthetic aperture radar (SAR) despecklingproblem with the necessity of neither clean (speckle-free) SAR images norindependent speckled image pairs from the same scene, a practical solution forSAR despeckling (PSD) is proposed. Firstly, to generate speckled-to-speckled(S2S) image pairs from the same scene in the situation of only single speckledSAR images are available, an adversarial learning framework is designed. Then,the S2S SAR image pairs are employed to train a modified despecklingNested-UNet model using the Noise2Noise (N2N) strategy. Moreover, an iterativeversion of the PSD method (PSDi) is also proposed. The performance of theproposed methods is demonstrated by both synthetic speckled and real SAR data.SAR block-matching 3-D algorithm (SAR-BM3D) and SAR dilated residual network(SAR-DRN) are used in the visual and quantitative comparison. Experimentalresults show that the proposed methods can reach a good tradeoff betweenspeckle suppression and edge preservation."," Ye Yuan, Jianguo Sun, Jian Guan, Pengming Feng, Yanxia Wu",http://arxiv.org/abs/1912.06295
53,On Privatizing Equilibrium Computation in Aggregate Games over Networks.,"We propose a distributed algorithm to compute an equilibrium in aggregategames where players communicate over a fixed undirected network. Our algorithmexploits correlated perturbation to obfuscate information shared over thenetwork. We prove that our algorithm does not reveal private information ofplayers to an honest-but-curious adversary who monitors several nodes in thenetwork. In contrast with differential privacy based algorithms, our methoddoes not sacrifice accuracy of equilibrium computation to provide privacyguarantees."," Shripad Gade, Anna Winnicki, Subhonmesh Bose",http://arxiv.org/abs/1912.06296
54,Differential tomography of micromechanical evolution in elastic materials of unknown micro/macrostructure.,"Differential evolution indicators are introduced for 3D spatiotemporalimaging of micromechanical processes in complex materials where progressivevariations due to manufacturing and/or aging are housed in a highly scatteringbackground of a-priori unknown or uncertain structure. In this vein, athree-tier imaging platform is established where: (1) the domain isperiodically (or continuously) subject to illumination and sensing in anarbitrary configuration; (2) sequential sets of measured data are deployed todistill segment-wise scattering signatures of the domain&apos;s internal structurethrough carefully constructed, non-iterative solutions to the scatteringequation; and (3) the resulting solution sequence is then used to rigorouslyconstruct an imaging functional carrying appropriate invariance with respect tothe unknown stationary components of the background e.g., pre-existinginterstitial boundaries and bubbles. This gives birth to differentialindicators that specifically recover the 3D support of micromechanicalevolution within a network of unknown scatterers. The direct scattering problemis formulated in the frequency domain where the background is comprised of arandom distribution of monolithic fragments. The constituents are connected viahighly heterogeneous interfaces of unknown elasticity and dissipation which aresubject to spatiotemporal evolution. The support of internal boundaries aresequentially illuminated by a set of incident waves and thus-induced scatteredfields are captured over a generic observation surface. The performance of theproposed imaging indicator is illustrated through a set of numericalexperiments for spatiotemporal reconstruction of progressive damage zonesfeaturing randomly distributed cracks and bubbles."," Fatemeh Pourahmadian, Houssem Haddar",http://arxiv.org/abs/1912.06298
55,The SBP Algorithm for Maximizing Revenue in Online Dial-a-Ride.,"In the Online-Dial-a-Ride Problem (OLDARP) a server travels through a metricspace to serve requests for rides. We consider a variant where each requestspecifies a source, destination, release time, and revenue that is earned forserving the request. The goal is to maximize the total revenue earned within agiven time limit. We prove that no non-preemptive deterministic onlinealgorithm for OLDARP can be guaranteed to earn more than twice the revenueearned by an optimal offline solution. We then investigate the\textsc{segmented best path} ($SBP$) algorithm of~\cite{atmos17} for thegeneral case of weighted graphs. The previously-established lower and upperbounds for the competitive ratio of $SBP$ are 4 and 6, respectively, underreasonable assumptions about the input instance. We eliminate the gap byproving that the competitive ratio is 5 (under the same reasonableassumptions). We also prove that when revenues are uniform, $SBP$ hascompetitive ratio 4. Finally, we provide a competitive analysis of $SBP$ oncomplete bipartite graphs."," Ananya Christman, Christine Chung, Nicholas Jaczko, Tianzhi Li, Scott Westvold, Xinyue Xu",http://arxiv.org/abs/1912.06300
56,Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning.,"Reinforcement learning, evolutionary algorithms and imitation learning arethree principal methods to deal with continuous control tasks. Reinforcementlearning is sample efficient, yet sensitive to hyper-parameters setting andneeds efficient exploration; Evolutionary algorithms are stable, but with lowsample efficiency; Imitation learning is both sample efficient and stable,however it requires the guidance of expert data. In this paper, we proposeRecruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning,a scalable framework that combines advantages of the three methods mentionedabove. The core of this framework is a dual-actors and single criticreinforcement learning agent. This agent can recruit high-fitness actors fromthe population of evolutionary algorithms, which instructs itself to learn fromexperience replay buffer. At the same time, low-fitness actors in theevolutionary population can imitate behavior patterns of the reinforcementlearning agent and improve their adaptability. Reinforcement and imitationlearners in this framework can be replaced with any off-policy actor-criticreinforcement learner or data-driven imitation learner. We evaluate RIM on aseries of benchmarks for continuous control tasks in Mujoco. The experimentalresults show that RIM outperforms prior evolutionary or reinforcement learningmethods. The performance of RIM&apos;s components is significantly better thancomponents of previous evolutionary reinforcement learning algorithm, and therecruitment using soft update enables reinforcement learning agent to learnfaster than that using hard update."," Shuai L&#xfc;, Shuai Han, Wenbo Zhou, Junwei Zhang",http://arxiv.org/abs/1912.06310
57,Short-duration Speaker Verification (SdSV) Challenge 2020: the Challenge Evaluation Plan.,This document describes task1 of the Short-Duration Speaker VerificationChallenge (SdSVC) 2020. The main aim of the challenge is to evaluate newtechnologies for text-dependent speaker verification (TD-SV). There is one moretask in the SdSVC which is text-independent speaker verification which isexplained in a separate description file. The evaluation dataset in thechallenge is recently released multi-purpose DeepMine dataset. The dataset hasthree parts and among them part1 is for text-dependent speaker verification.," Hossein Zeinali, Kong Aik Lee, Jahangir Alam, Lukas Burget",http://arxiv.org/abs/1912.06311
58,Identity Preserve Transform: Understand What Activity Classification Models Have Learnt.,"Activity classification has observed great success recently. The performanceon small dataset is almost saturated and people are moving towards largerdatasets. What leads to the performance gain on the model and what the modelhas learnt? In this paper we propose identity preserve transform (IPT) to studythis problem. IPT manipulates the nuisance factors (background, viewpoint,etc.) of the data while keeping those factors related to the task (humanmotion) unchanged. To our surprise, we found popular models are using highlycorrelated information (background, object) to achieve high classificationaccuracy, rather than using the essential information (human motion). This canexplain why an activity classification model usually fails to generalize todatasets it is not trained on. We implement IPT in two forms, i.e. image-spacetransform and 3D transform, using synthetic images. The tool will be madeopen-source to help study model and dataset design."," Jialing Lyu, Weichao Qiu, Xinyue Wei, Yi Zhang, Alan Yuille, Zheng-Jun Zha",http://arxiv.org/abs/1912.06314
59,Grounding-Tracking-Integration.,"In this paper, we study tracking by language that localizes the target boxsequence in a video based on a language query. We propose a framework calledGTI that decomposes the problem into three sub-tasks: Grounding, Tracking andIntegration. The three sub-task modules operate simultaneously and predict thebox sequence frame-by-frame. ""Grounding"" predicts the referred region directlyfrom the language query. ""Tracking"" localizes the target based on the historyof the grounded regions in previous frames. ""Integration"" generates finalpredictions by synergistically combining grounding and tracking. With the""integration"" task as the key, we explore how to indicate the quality of thegrounded regions in each frame and achieve the desired mutually beneficialcombination. To this end, we propose an ""RT-integration"" method that definesand predicts two scores to guide the integration: 1) R-score represents theRegion correctness whether the grounding prediction accurately covers thetarget, and 2) T-score represents the Template quality whether the regionprovides informative visual cues to improve tracking in future frames. Wepresent our real-time GTI implementation with the proposed RT-integration, andbenchmark the framework on LaSOT and Lingual OTB99 with highly promisingresults. Moreover, a disambiguated version of LaSOT queries can be used tofacilitate future tracking by language studies."," Zhengyuan Yang, Tushar Kumar, Tianlang Chen, Jiebo Luo",http://arxiv.org/abs/1912.06316
60,Small Object Detection using Context and Attention.,"There are many limitations applying object detection algorithm on variousenvironments. Especially detecting small objects is still challenging becausethey have low resolution and limited information. We propose an objectdetection method using context for improving accuracy of detecting smallobjects. The proposed method uses additional features from different layers ascontext by concatenating multi-scale features. We also propose object detectionwith attention mechanism which can focus on the object in image, and it caninclude contextual information from target layer. Experimental results showsthat proposed method also has higher accuracy than conventional SSD ondetecting small objects. Also, for 300$\times$300 input, we achieved 78.1% MeanAverage Precision (mAP) on the PASCAL VOC2007 test set."," Jeong-Seon Lim, Marcella Astrid, Seung-Ik Lee, Hyun-Jin Yoon",http://arxiv.org/abs/1912.06319
61,Are We Making Real Progress in Simulated Environments? Measuring the Sim2Real Gap in Embodied Visual Navigation.,"Does progress in simulation translate to progress in robotics? Specifically,if method A outperforms method B in simulation, how likely is the trend to holdin reality on a robot? We examine this question for embodied (PointGoal)navigation, developing engineering tools and a research paradigm for evaluatinga simulator by its sim2real predictivity, revealing surprising findings aboutprior work. First, we develop Habitat-PyRobot Bridge (HaPy), a library forseamless execution of identical code on a simulated agent and a physical robot.Habitat-to-Locobot transfer with HaPy involves just one line change in config,essentially treating reality as just another simulator! Second, we investigatesim2real predictivity of Habitat-Sim for PointGoal navigation. We 3D-scan aphysical lab space to create a virtualized replica, and run parallel tests of 9different models in reality and simulation. We present a new metric calledSim-vs-Real Correlation Coefficient (SRCC) to quantify sim2real predictivity.</p><p>Our analysis reveals several important findings. We find that SRCC forHabitat as used for the CVPR19 challenge is low (0.18 for the success metric),which suggests that performance improvements for this simulator-based challengewould not transfer well to a physical robot. We find that this gap is largelydue to AI agents learning to &apos;cheat&apos; by exploiting simulator imperfections:specifically, the way Habitat allows for &apos;sliding&apos; along walls on collision.Essentially, the virtual robot is capable of cutting corners, leading tounrealistic shortcuts through non-navigable spaces. Naturally, such exploits donot work in the real world where the robot stops on contact with walls. Ourexperiments show that it is possible to optimize simulation parameters toenable robots trained in imperfect simulators to generalize learned skills toreality (e.g. improving $SRCC_{Succ}$ from 0.18 to 0.844)."," Abhishek Kadian, Joanne Truong, Aaron Gokaslan, Alexander Clegg, Erik Wijmans, Stefan Lee, Manolis Savva, Sonia Chernova, Dhruv Batra",http://arxiv.org/abs/1912.06321
62,Queueing Analysis of GPU-Based Inference Servers with Dynamic Batching: A Closed-Form Characterization.,"GPU-accelerated computing is a key technology to realize high-speed inferenceservers using deep neural networks (DNNs). An important characteristic ofGPU-based inference is that the computational efficiency, in terms of theprocessing speed and energy consumption, drastically increases by processingmultiple jobs together in a batch. In this paper, we formulate GPU-basedinference servers as a batch service queueing model with batch-size dependentprocessing times. We first show that the energy efficiency of the servermonotonically increases with the arrival rate of inference jobs, which suggeststhat it is energy-efficient to operate the inference server under a utilizationlevel as high as possible within a latency requirement of inference jobs. Wethen derive a closed-form upper bound for the mean latency, which provides asimple characterization of the latency performance. Through simulation andnumerical experiments, we show that the exact value of the mean latency is wellapproximated by this upper bound.", Yoshiaki Inoue,http://arxiv.org/abs/1912.06322
63,Down to the Last Detail: Virtual Try-on with Detail Carving.,"Virtual try-on under arbitrary poses has attracted lots of research attentiondue to its huge potential applications. However, existing methods can hardlypreserve the details in clothing texture and facial identity (face, hair) whilefitting novel clothes and poses onto a person. In this paper, we propose anovel multi-stage framework to synthesize person images, where rich details insalient regions can be well preserved. Specifically, a multi-stage framework isproposed to decompose the generation into spatial alignment followed by acoarse-to-fine generation. To better preserve the details in salient areas suchas clothing and facial areas, we propose a Tree-Block (tree dilated fusionblock) to harness multi-scale features in the generator networks. Withend-to-end training of multiple stages, the whole framework can be jointlyoptimized for results with significantly better visual fidelity and richerdetails. Extensive experiments on standard datasets demonstrate that ourproposed framework achieves the state-of-the-art performance, especially inpreserving the visual details in clothing texture and facial identity. Ourimplementation will be publicly available soon."," Jiahang Wang, Wei Zhang, Weizhong Liu, Tao Mei",http://arxiv.org/abs/1912.06324
64,Toward Automatic Threat Recognition for Airport X-ray Baggage Screening with Deep Convolutional Object Detection.,"For the safety of the traveling public, the Transportation SecurityAdministration (TSA) operates security checkpoints at airports in the UnitedStates, seeking to keep dangerous items off airplanes. At these checkpoints,the TSA employs a fleet of X-ray scanners, such as the Rapiscan 620DV, soTransportation Security Officers (TSOs) can inspect the contents of carry-onpossessions. However, identifying and locating all potential threats can be achallenging task. As a result, the TSA has taken a recent interest in deeplearning-based automated detection algorithms that can assist TSOs. In acollaboration funded by the TSA, we collected a sizable new dataset of X-rayscans with a diverse set of threats in a wide array of contexts, trainedseveral deep convolutional object detection models, and integrated such modelsinto the Rapiscan 620DV, resulting in functional prototypes capable ofoperating in real time. We show performance of our models on held-outevaluation sets, analyze several design parameters, and demonstrate thepotential of such systems for automated detection of threats that can be foundin airports."," Kevin J Liang, John B. Sigman, Gregory P. Spell, Dan Strellis, William Chang, Felix Liu, Tejas Mehta, Lawrence Carin",http://arxiv.org/abs/1912.06329
65,A Guide to Design Disturbance Observer.,"The goal of this paper is to clarify the robustness and performanceconstraints in the design of control systems based on disturbance observer(DOB). Although the bandwidth constraints of a DOB have long been verywell-known by experiences and observations, they have not been formulated andclearly reported yet. In this regard, the Bode and Poisson integral formulasare utilized in the robustness analysis so that the bandwidth constraints of aDOB are derived analytically. In this paper, it is shown that the bandwidth ofa DOB has upper and lower bounds to obtain good robustness if the plant hasnon-minimum phase zero(s) and pole(s), respectively. Besides that theperformance of a system can be improved by using a higher-order disturbanceobserver (HODOB); however, the robustness may deteriorate, and the bandwidthconstraints become more severe. New analysis and design methods, which providegood robustness and predefined performance criteria, are proposed for the DOBbased robust control systems. The validity of the proposals are verified bysimulation results."," Emre Sariyildiz, Kouhei Ohnishi",http://arxiv.org/abs/1912.06331
66,TopoAct: Exploring the Shape of Activations in Deep Learning.,"Deep neural networks such as GoogLeNet and ResNet have achieved superhumanperformance in tasks like image classification. To understand how such superiorperformance is achieved, we can probe a trained deep neural network by studyingneuron activations, that is, combinations of neuron firings, at any layer ofthe network in response to a particular input. With a large set of inputimages, we aim to obtain a global view of what neurons detect by studying theiractivations. We ask the following questions: What is the shape of the space ofactivations? That is, what is the organizational principle behind neuronactivations, and how are the activations related within a layer and acrosslayers? Applying tools from topological data analysis, we present TopoAct, avisual exploration system used to study topological summaries of activationvectors for a single layer as well as the evolution of such summaries acrossmultiple layers. We present visual exploration scenarios using TopoAct thatprovide valuable insights towards learned representations of an imageclassifier."," Archit Rathore, Nithin Chalapathi, Sourabh Palande, Bei Wang",http://arxiv.org/abs/1912.06332
67,An Adaptive Reaction Force Observer Design.,"In this paper, a new adaptive design method is proposed for reaction forceobserver (RFOB) based robust force control systems. It is a well-known factthat an RFOB has several superiorities over a force sensor such as higher forcecontrol bandwidth, stability improvement, force-sensorless force control, andso on. However, there are insufficient analysis and design methods for an RFOBbased robust force control system; therefore, its stability and performancehighly depend on designers own experiences. To overcome this issue, newstability analysis and novel adaptive design methods are proposed for RFOBbased robust force control systems. In the proposed adaptive design method, thedesign parameters of the robust force control system, i.e., the bandwidths of adisturbance observer (DOB) and an RFOB, the nominal and identified inertias inthe design of a DOB and an RFOB, respectively, and the force control gain, areadjusted automatically by using an adaptive control algorithm which is derivedby estimating the plant parameters and environmental impedance. The proposedadaptive design method provides good stability and performance by consideringthe design constraints of a DOB. The validity of the proposals is verified bysimulation and experimental results."," Emre Sariyildiz, Kouhei Ohnishi",http://arxiv.org/abs/1912.06333
68,Uncertainty Visualization of 2D Morse Complex Ensembles Using Statistical Summary Maps.,"Morse complexes are gradient-based topological descriptors with closeconnections to Morse theory. They are widely applicable in scientificvisualization as they serve as important abstractions for gaining insights intothe topology of scalar fields. Noise inherent to scalar field data due toacquisitions and processing, however, limits our understanding of the Morsecomplexes as structural abstractions. We, therefore, explore uncertaintyvisualization of an ensemble of 2D Morse complexes that arise from scalarfields coupled with data uncertainty. We propose statistical summary maps asnew entities for capturing structural variations and visualizing positionaluncertainties of Morse complexes in ensembles. Specifically, we introduce twotypes of statistical summary maps -- the Probabilistic Map and the Survival Map-- to characterize the uncertain behaviors of local extrema and local gradientflows, respectively. We demonstrate the utility of our proposed approach usingsynthetic and real-world datasets."," Tushar Athawale, Dan Maljovec, Chris R. Johnson, Valerio Pascucci, Bei Wang",http://arxiv.org/abs/1912.06341
69,MM Algorithms for Distance Covariance based Sufficient Dimension Reduction and Sufficient Variable Selection.,"Sufficient dimension reduction (SDR) using distance covariance (DCOV) wasrecently proposed as an approach to dimension-reduction problems. Compared withother SDR methods, it is model-free without estimating link function and doesnot require any particular distributions on predictors (see Sheng and Yin,2013, 2016). However, the DCOV-based SDR method involves optimizing a nonsmoothand nonconvex objective function over the Stiefel manifold. To tackle thenumerical challenge, we novelly formulate the original objective functionequivalently into a DC (Difference of Convex functions) program and constructan iterative algorithm based on the majorization-minimization (MM) principle.At each step of the MM algorithm, we inexactly solve the quadratic subproblemon the Stiefel manifold by taking one iteration of Riemannian Newton&apos;s method.The algorithm can also be readily extended to sufficient variable selection(SVS) using distance covariance. We establish the convergence property of theproposed algorithm under some regularity conditions. Simulation studies showour algorithm drastically improves the computation efficiency and is robustacross various settings compared with the existing method. Supplementalmaterials for this article are available."," Runxiong Wu, Xin Chen",http://arxiv.org/abs/1912.06342
70,Maintaining Ferment: On Opinion Control Over Social Networks.,"We consider the design of external inputs to achieve a control objective onthe opinions, represented by scalars, in a social network. The opinion dynamicsfollow a variant of the discrete-time Friedkin-Johnsen model. We first considertwo minimum cost optimal control problems over a finite interval (T0, T) : (1)TF where opinions at all nodes should exceed a given level; and (2) GF where ascalar function of the opinion vector should exceed a given level. For bothproblems we first provide a Pontryagin maximum principle (PMP) based controlfunction when the controllable nodes are specified. We then show that boththese problems exhibit the turnpike property where both the control functionand the state vectors stay near their equilibrium for a large fraction of thetime. This property is then used to choose the optimum set of controllablenodes. We then consider a third system, MF, which is a cost-constrained optimalcontrol problem where we maximize the minimum value of a scalar function of theopinion vector over (T0, T). We provide a numerical algorithm to derive thecontrol function for this problem using non-smooth PMP based techniques.Extensive numerical studies illustrate the three models, control techniques andcorresponding outcomes."," Mohak Goyal, Nikhil Karamchandani, Debasish Chatterjee, D. Manjunath",http://arxiv.org/abs/1912.06343
71,A Method for Arbitrary Instance Style Transfer.,"The ability to synthesize style and content of different images to form avisually coherent image holds great promise in various applications such asstylistic painting, design prototyping, image editing, and augmented reality.However, the majority of works in image style transfer have focused ontransferring the style of an image to the entirety of another image, and only avery small number of works have experimented on methods to transfer style to aninstance of another image. Researchers have proposed methods to circumvent thedifficulty of transferring style to an instance in an arbitrary shape. In thispaper, we propose a topologically inspired algorithm called Forward Stretchingto tackle this problem by transforming an instance into a tensorrepresentation, which allows us to transfer style to this instance itselfdirectly. Forward Stretching maps pixels to specific positions and interpolatevalues between pixels to transform an instance to a tensor. This algorithmallows us to introduce a method to transfer arbitrary style to an instance inan arbitrary shape. We showcase the results of our method in this paper."," Zhifeng Yu, Yusheng Wu, Tianyou Wang",http://arxiv.org/abs/1912.06347
72,Learned Video Compression via Joint Spatial-Temporal Correlation Exploration.,"Traditional video compression technologies have been developed over decadesin pursuit of higher coding efficiency. Efficient temporal informationrepresentation plays a key role in video coding. Thus, in this paper, wepropose to exploit the temporal correlation using both first-order optical flowand second-order flow prediction. We suggest an one-stage learning approach toencapsulate flow as quantized features from consecutive frames which is thenentropy coded with adaptive contexts conditioned on joint spatial-temporalpriors to exploit second-order correlations. Joint priors are embedded inautoregressive spatial neighbors, co-located hyper elements and temporalneighbors using ConvLSTM recurrently. We evaluate our approach for thelow-delay scenario with High-Efficiency Video Coding (H.265/HEVC), H.264/AVCand another learned video compression method, following the common testsettings. Our work offers the state-of-the-art performance, with consistentgains across all popular test sequences."," Haojie Liu, Han shen, Lichao Huang, Ming Lu, Tong Chen, Zhan Ma",http://arxiv.org/abs/1912.06348
73,Random Access with Opportunity Detection in Wireless Networks.,"This letter proposes a novel random medium access control (MAC) based on atransmission opportunity prediction, which can be measured in a form of aconditional success probability given transmitter-side interference. Atransmission probability depends on the opportunity prediction, preventingindiscriminate transmissions and reducing excessive interference causingcollisions. Using stochastic geometry, we derive a fixed-point equation toprovide the optimal transmission probability maximizing a proportionally fairthroughput. Its approximated solution is given in closed form. The proposed MACis applicable to full-duplex networks, leading to significant throughputimprovement by allowing more nodes to transmit."," Jinho Choi, Seung-Woo Ko, Koji Yamamoto, Seong-Lyun Kim",http://arxiv.org/abs/1912.06352
74,Elastic registration based on compliance analysis and biomechanical graph matching.,"An automatic elastic registration method suited for vascularized organs isproposed. The vasculature in both the preoperative and intra-operative imagesis represented as a graph. A typical application of this method is the fusionof pre-operative information onto the organ during surgery, to compensate forthe limited details provided by the intra-operative imaging modality (e.g.CBCT) and to cope with changes in the shape of the organ. Due to imagemodalities differences and organ deformation, each graph has a differenttopology and shape. The Adaptive Compliance Graph Matching (ACGM) methodpresented does not require any manual initialization, handles intra-operativenonrigid deformations of up to 65 mm and computes a complete displacement fieldover the organ from only the matched vasculature. ACGM is better than theprevious Biomechanical Graph Matching method 3 (BGM) because it uses anefficient biomechanical vascularized liver model to compute the organ&apos;stransformation and the vessels bifurcations compliance. This allows toefficiently find the best graph matches with a novel compliance-based adaptivesearch. These contributions are evaluated on ten realistic synthetic and tworeal porcine automatically segmented datasets. ACGM obtains better targetregistration error (TRE) than BGM, with an average TRE in the real datasets of4.2 mm compared to 6.5 mm, respectively. It also is up to one order ofmagnitude faster, less dependent on the parameters used and more robust tonoise."," Jaime Garcia Guevara (MIMESIS), Igor Peterlik (IHU Strasbourg), Marie-Odile Berger (MAGRIT), St&#xe9;phane Cotin (MIMESIS)",http://arxiv.org/abs/1912.06353
75,Bonn Activity Maps: Dataset Description.,"The key prerequisite for accessing the huge potential of current machinelearning techniques is the availability of large databases that capture thecomplex relations of interest. Previous datasets are focused on either 3D scenerepresentations with semantic information, tracking of multiple persons andrecognition of their actions, or activity recognition of a single person incaptured 3D environments. We present Bonn Activity Maps, a large-scale datasetfor human tracking, activity recognition and anticipation of multiple persons.Our dataset comprises four different scenes that have been recorded bytime-synchronized cameras each only capturing the scene partially, thereconstructed 3D models with semantic annotations, motion trajectories forindividual people including 3D human poses as well as human activityannotations. We utilize the annotations to generate activity likelihoods on the3D models called activity maps."," Julian Tanke, Oh-Hun Kwon, Patrick Stotko, Radu Alexandru Rosu, Michael Weinmann, Hassan Errami, Sven Behnke, Maren Bennewitz, Reinhard Klein, Andreas Weber, Angela Yao, Juergen Gall",http://arxiv.org/abs/1912.06354
76,On Pre-transformed Polar Codes.,"In this paper, we prove that any pre-transformation with an upper-triangularmatrix (including cyclic redundancy check (CRC), parity-check (PC) andconvolution code (CC) matrix) does not reduce the code minimum distance and anproperly designed pre-transformation can reduce the number of codewords withthe minimum distance. This explains that the pre-transformed polar codes canperform better than the Polar/RM codes."," Bin Li, Huazi Zhang, Jiaqi Gu",http://arxiv.org/abs/1912.06359
77,Construction and Maintenance of Swarm Drones.,"In this paper we study the dynamic version of the covering problem motivatedby the coverage of drones&apos; swarm: Let $S$ be a set of $n$ non-negative weightedpoints in the plane representing users. Also, consider a set $P$ of $m$ disksthat correspond to the covering radius of each drone. We want to place (andmaintain) set $P$ such that the sum of the weights of the points in $S$ coveredby disks from $P$ is maximized. We present a data structure that maintains asmall constant factor approximate solution efficiently, under insertions anddeletions of points to/from $S$ where each update operation can be performed$O(\log n)$ time."," Kiril Danilchenko, Michael Segal",http://arxiv.org/abs/1912.06360
78,RSSI-based Secure Localization in the Presence of Malicious Nodes in Sensor Networks.,"The ability of a sensor node to determine its location in a sensor network isimportant in many applications. The infrastructure for the location-basedservices is an easy target for malicious attacks. We address scenarios wheremalicious node(s) attempt to disrupt, in an uncoordinated or coordinatedmanner, the localization process of a target node. We propose four techniquesfor secure localization: weighted least square (WLS), secure weighted leastsquare (SWLS), and $\ell_1$-norm based techniques LN-1 and LN-1E, in a networkthat includes one or more compromised anchor nodes. WLS and SWLS techniques areshown to offer significant advantage over existing techniques by assigninglarger weights to the anchor nodes that are closer to the target node, and bydetecting the malicious nodes and eliminating their measurements from thelocalization process. In a coordinated attack, the localization problem can beposed as a plane fitting problem where the measurements from non-malicious andmalicious anchor nodes lie on two different planes. LN-1E technique estimatesthese two planes and prevents disruption of the localization process. TheCramer-Rao lower bound (CRLB) for the position estimate is also derived. Theproposed techniques are shown to provide better localization accuracy than theexisting algorithms."," Bodhibrata Mukhopadhyay, Seshan Srirangarajan, Subrat Kar",http://arxiv.org/abs/1912.06362
79,Fast Image Caption Generation with Position Alignment.,"Recent neural network models for image captioning usually employ anencoder-decoder architecture, where the decoder adopts a recursive sequencedecoding way. However, such autoregressive decoding may result in sequentialerror accumulation and slow generation which limit the applications inpractice. Non-autoregressive (NA) decoding has been proposed to cover theseissues but suffers from language quality problem due to the indirect modelingof the target distribution. Towards that end, we propose an improved NAprediction framework to accelerate image captioning. Our decoding part consistsof a position alignment to order the words that describe the content detectedin the given image, and a fine non-autoregressive decoder to generate elegantdescriptions. Furthermore, we introduce an inference strategy that regardsposition information as a latent variable to guide the further sentencegeneration. The Experimental results on public datasets show that our proposedmodel achieves better performance compared to general NA captioning models,while achieves comparable performance as autoregressive image captioning modelswith a significant speedup.", Zheng-cong Fei,http://arxiv.org/abs/1912.06365
80,Provably Efficient Reinforcement Learning with Aggregated States.,"We establish that an optimistic variant of Q-learning applied to afinite-horizon episodic Markov decision process with an aggregated staterepresentation incurs regret $\tilde{\mathcal{O}}(\sqrt{H^5 M K} + \epsilonHK)$, where $H$ is the horizon, $M$ is the number of aggregate states, $K$ isthe number of episodes, and $\epsilon$ is the largest difference between anypair of optimal state-action values associated with a common aggregate state.Notably, this regret bound does not depend on the number of states or actions.To the best of our knowledge, this is the first such result pertaining to areinforcement learning algorithm applied with nontrivial value functionapproximation without any restrictions on the Markov decision process."," Shi Dong, Benjamin Van Roy, Zhengyuan Zhou",http://arxiv.org/abs/1912.06366
81,Toward an Automated Auction Framework for Wireless Federated Learning Services Market.,"In traditional machine learning, the central server first collects the dataowners&apos; private data together and then trains the model. However, people&apos;sconcerns about data privacy protection are dramatically increasing. Theemerging paradigm of federated learning efficiently builds machine learningmodels while allowing the private data to be kept at local devices. The successof federated learning requires sufficient data owners to jointly utilize theirdata, computing and communication resources for model training. In this paper,we propose an auction based market model for incentivizing data owners toparticipate in federated learning. We design two auction mechanisms for thefederated learning platform to maximize the social welfare of the federatedlearning services market. Specifically, we first design an approximatestrategy-proof mechanism which guarantees the truthfulness, individualrationality, and computational efficiency. To improve the social welfare, wedevelop an automated strategy-proof mechanism based on deep reinforcementlearning and graph neural networks. The communication traffic congestion andthe unique characteristics of federated learning are particularly considered inthe proposed model. Extensive experimental results demonstrate that ourproposed auction mechanisms can efficiently maximize the social welfare andprovide effective insights and strategies for the platform to organize thefederated training."," Yutao Jiao, Ping Wang, Dusit Niyato, Bin Lin, Dong In Kim",http://arxiv.org/abs/1912.06370
82,Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching.,"The deep multi-view stereo (MVS) and stereo matching approaches generallyconstruct 3D cost volumes to regularize and regress the output depth ordisparity. These methods are limited when high-resolution outputs are neededsince the memory and time costs grow cubically as the volume resolutionincreases. In this paper, we propose a both memory and time efficient costvolume formulation that is complementary to existing multi-view stereo andstereo matching approaches based on 3D cost volumes. First, the proposed costvolume is built upon a standard feature pyramid encoding geometry and contextat gradually finer scales. Then, we can narrow the depth (or disparity) rangeof each stage by the depth (or disparity) map from the previous stage. Withgradually higher cost volume resolution and adaptive adjustment of depth (ordisparity) intervals, the output is recovered in a coarser to fine manner.</p><p>We apply the cascade cost volume to the representative MVS-Net, and obtain a23.1% improvement on DTU benchmark (1st place), with 50.6% and 74.2% reductionin GPU memory and run-time. It is also the state-of-the-art learning-basedmethod on Tanks and Temples benchmark. The statistics of accuracy, run-time andGPU memory on other representative stereo CNNs also validate the effectivenessof our proposed method."," Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, Ping Tan",http://arxiv.org/abs/1912.06378
83,A Gap Analysis of Low-Cost Outdoor Air Quality Sensor In-Field Calibration.,"In recent years, interest in monitoring air quality has been growing.Traditional environmental monitoring stations are very expensive, both toacquire and to maintain, therefore their deployment is generally very sparse.This is a problem when trying to generate air quality maps with a fine spatialresolution. Given the general interest in air quality monitoring, low-cost airquality sensors have become an active area of research and development.Low-cost air quality sensors can be deployed at a finer level of granularitythan traditional monitoring stations. Furthermore, they can be portable andmobile. Low-cost air quality sensors, however, present some challenges: theysuffer from cross-sensitivities between different ambient pollutants; they canbe affected by external factors such as traffic, weather changes, and humanbehavior; and their accuracy degrades over time. Some promising machinelearning approaches can help us obtain highly accurate measurements withlow-cost air quality sensors. In this article, we present low-cost sensortechnologies, and we survey and assess machine learning-based calibrationtechniques for their calibration. We conclude by presenting open questions anddirections for future research."," Francesco Concas, Julien Mineraud, Eemil Lagerspetz, Samu Varjonen, Kai Puolam&#xe4;ki, Petteri Nurmi, Sasu Tarkoma",http://arxiv.org/abs/1912.06384
84,Seizure Prediction Using Bidirectional LSTM.,"Approximately, 50 million people in the world are affected by epilepsy. Forpatients, the anti-epileptic drugs are not always useful and these drugs mayhave undesired side effects on a patient&apos;s health. If the seizure is predictedthe patients will have enough time to take preventive measures. The purpose ofthis work is to investigate the application of bidirectional LSTM for seizureprediction. In this paper, we trained EEG data from canines on a doubleBidirectional LSTM layer followed by a fully connected layer. The data wasprovided in the form of a Kaggle competition by American Epilepsy Society. Themain task was to classify the interictal and preictal EEG clips. Using thismodel, we obtained an AUC of 0.84 on the test dataset. Which shows that ourclassifier&apos;s performance is above chance level on unseen data. The comparisonwith the previous work shows that the use of bidirectional LSTM networks canachieve significantly better results than SVM and GRU networks."," Hazrat Ali, Feroz Karim, Junaid Javed Qureshi, Adnan Omer Abuassba, Mohammad Farhad Bulbul",http://arxiv.org/abs/1912.06385
85,A multimesh finite element method for the Navier-Stokes equations based on projection methods.,"The multimesh finite element method is a technique for solving partialdifferential equations on multiple non-matching meshes by enforcing interfaceconditions using Nitsche&apos;s method. Since the non-matching meshes can result inarbitrarily cut cells, additional stabilization terms are needed to obtain astable variational formulation. In this contribution we extend the multimeshfinite element method to the Navier-Stokes equations based on the incrementalpressure correction scheme. For each step in the pressure correction scheme, wederive a multimesh finite element formulation with suitable stabilizationterms. The overall scheme yields expected spatial and temporal convergencerates on the Taylor-Green problem, and demonstrates good agreement for the dragand lift coefficients on the Turek-Schafer benchmark (DFG benchmark 2D-3).Finally, we illustrate the capabilities of the proposed scheme by optimizingthe layout of obstacles in a channel."," J&#xf8;rgen S. Dokken, August Johansson, Andr&#xe9; Massing, Simon W. Funke",http://arxiv.org/abs/1912.06392
86,Neural Cages for Detail-Preserving 3D Deformations.,"We propose a novel learnable representation for detail-preserving shapedeformation. The goal of our method is to warp a source shape to match thegeneral structure of a target shape, while preserving the surface details ofthe source. Our method extends a traditional cage-based deformation technique,where the source shape is enclosed by a coarse control mesh termed \emph{cage},and translations prescribed on the cage vertices are interpolated to any pointon the source mesh via special weight functions. The use of this sparse cagescaffolding enables preserving surface details regardless of the shape&apos;sintricacy and topology. Our key contribution is a novel neural networkarchitecture for predicting deformations by controlling the cage. Weincorporate a differentiable cage-based deformation module in our architecture,and train our network end-to-end. Our method can be trained with commoncollections of 3D models in an unsupervised fashion, without any cage-specificannotations. We demonstrate the utility of our method for synthesizing shapevariations and deformation transfer."," Wang Yifan, Noam Aigerman, Vladimir Kim, Siddhartha Chaudhuri, Olga Sorkine-Hornung",http://arxiv.org/abs/1912.06395
87,Overlapping Multi-Patch Isogeometric Method with Minimal Stabilization.,"We present a novel method for isogeometric analysis (IGA) to directly work ongeometries constructed by Boolean operations including difference (i.e.,trimming), union and intersection. Particularly, this work focuses on the unionoperation, which involves multiple independent, generally non-conforming andtrimmed spline patches. Given a series of patches, we overlay one on top ofanother in a certain order. While the invisible part of each patch is trimmedaway, the visible parts of all the patches constitute the entire computationaldomain. We employ the Nitsche&apos;s method to weakly couple independent patchesthrough visible interfaces. Moreover, we propose a minimal stabilization methodto address the instability issue that arises on the interfaces shared by smalltrimmed elements. We show in theory that our proposed method recovers stabilityand guarantees well-posedness of the problem as well as optimal errorestimates. In the end, we numerically verify the theory by solving thePoisson&apos;s equation on various geometries that are obtained by the unionoperation."," Pablo Antolin, Annalisa Buffa, Riccardo Puppi, Xiaodong Wei",http://arxiv.org/abs/1912.06400
88,Two-User SIMO Interference Channel with TIN: Improper Signaling versus Time-Sharing.,"In the two-user Gaussian interference channel with Gaussian inputs andtreating interference as noise (TIN), improper complex signals can bebeneficial if time-sharing is not allowed or if only the data rates areaveraged over several transmit strategies (convex hull formulation). On theother hand, proper (circularly symmetric) signals have recently been shown tobe optimal if coded time-sharing is considered, i.e., if both the data ratesand the transmit powers are averaged. In this paper, we show that bothconclusions remain the same if single-input multiple-output (SIMO) systems withmultiple antennas at the receivers are considered. The proof for the case withcoded time-sharing is via a novel enhanced channel concept for the two-userSIMO interference channel, which turns out to deliver a tight outer bound tothe TIN rate region with coded time-sharing. The result for the case withoutcoded time-sharing is demonstrated by studying specific examples in which anewly proposed composite real gradient-projection method for improper signalingcan outperform the globally optimal proper signaling strategy. In addition, wediscuss how the achievable TIN rate region with coded time-sharing can becomputed numerically."," Christoph Hellings, Ferhad Askerbeyli, Wolfgang Utschick",http://arxiv.org/abs/1912.06402
89,Real-time texturing for 6D object instance detection from RGB Images.,"For objected detection, the availability of color cues strongly influencesdetection rates and is even a prerequisite for many methods. However, whentraining on synthetic CAD data, this information is not available. We thereforepresent a method for generating a texture-map from image sequences inreal-time. The method relies on 6 degree-of-freedom poses and a 3D-model beingavailable. In contrast to previous works this allows interleaving detection andtexturing for upgrading the detector on-the-fly. Our evaluation shows that theacquired texture-map significantly improves detection rates using the LINEMODdetector on RGB images only. Additionally, we use the texture-map todifferentiate instances of the same object by surface color."," Pavel Rojtberg, Arjan Kuijper",http://arxiv.org/abs/1912.06404
90,Understanding complex predictive models with Ghost Variables.,"We propose a procedure for assigning a relevance measure to each explanatoryvariable in a complex predictive model. We assume that we have a training setto fit the model and a test set to check the out of sample performance. First,the individual relevance of each variable is computed by comparing thepredictions in the test set, given by the model that includes all the variableswith those of another model in which the variable of interest is substituted byits ghost variable, defined as the prediction of this variable by using therest of explanatory variables. Second, we check the joint effects among thevariables by using the eigenvalues of a relevance matrix that is the covariancematrix of the vectors of individual effects. It is shown that in simple models,as linear or additive models, the proposed measures are related to standardmeasures of significance of the variables and in neural networks models (and inother algorithmic prediction models) the procedure provides information aboutthe joint and individual effects of the variables that is not usually availableby other methods. The procedure is illustrated with simulated examples and theanalysis of a large real data set."," Pedro Delicado, Daniel Pe&#xf1;a",http://arxiv.org/abs/1912.06407
91,Potential adversarial samples for white-box attacks.,"Deep convolutional neural networks can be highly vulnerable to smallperturbations of their inputs, potentially a major issue or limitation onsystem robustness when using deep networks as classifiers. In this paper wepropose a low-cost method to explore marginal sample data near trainedclassifier decision boundaries, thus identifying potential adversarial samples.By finding such adversarial samples it is possible to reduce the search spaceof adversarial attack algorithms while keeping a reasonable successfulperturbation rate. In our developed strategy, the potential adversarial samplesrepresent only 61% of the test data, but in fact cover more than 82% of theadversarial samples produced by iFGSM and 92% of the adversarial samplessuccessfully perturbed by DeepFool on CIFAR10."," Amir Nazemi, Paul Fieguth",http://arxiv.org/abs/1912.06409
92,On Profitability of Nakamoto double spend.,"Nakamoto double spend strategy, described in Bitcoin foundational article,leads to total ruin with positive probability and does not make sense from theprofitability point of view. The simplest strategy that can be profitableincorporates a stopping threshold when success is unlikely. We solve andcompute the exact profitability for this strategy. We compute the minimalamount of the double spend that is profitable. For a given amount of thetransaction, we determine the minimal number of confirmations to be requestedby the recipient such that this double spend strategy is non-profitable. Wefind that this number of confirmations is only 1 or 2 for average transactionsand a small hashrate of the attacker. This is substantially lower than theoriginal Nakamoto numbers that are widely used and are only based on thesuccess probability instead of the profitability."," Cyril Grunspan, Ricardo P&#xe9;rez-Marco",http://arxiv.org/abs/1912.06412
93,RDD-Eclat: Approaches to Parallelize Eclat Algorithm on Spark RDD Framework.,"Initially, a number of frequent itemset mining (FIM) algorithms have beendesigned on the Hadoop MapReduce, a distributed big data processing framework.But, due to heavy disk I/O, MapReduce is found to be inefficient for suchhighly iterative algorithms. Therefore, Spark, a more efficient distributeddata processing framework, has been developed with in-memory computation andresilient distributed dataset (RDD) features to support the iterativealgorithms. On the Spark RDD framework, Apriori and FP-Growth based FIMalgorithms have been designed, but Eclat-based algorithm has not been exploredyet. In this paper, RDD-Eclat, a parallel Eclat algorithm on the Spark RDDframework is proposed with its five variants. The proposed algorithms areevaluated on the various benchmark datasets, which shows that RDD-Eclatoutperforms the Spark-based Apriori by many times. Also, the experimentalresults show the scalability of the proposed algorithms on increasing thenumber of cores and size of the dataset."," Pankaj Singh, Sudhakar Singh, P. K. Mishra, Rakhi Garg",http://arxiv.org/abs/1912.06415
94,Deep Learning Algorithms for Coronary Artery Plaque Characterisation from CCTA Scans.,"Analysing coronary artery plaque segments with respect to their functionalsignificance and therefore their influence to patient management in anon-invasive setup is an important subject of current research. In this work wecompare and improve three deep learning algorithms for this task: A 3Drecurrent convolutional neural network (RCNN), a 2D multi-view ensembleapproach based on texture analysis, and a newly proposed 2.5D approach. Currentstate of the art methods utilising fluid dynamics based fractional flow reserve(FFR) simulation reach an AUC of up to 0.93 for the task of predicting anabnormal invasive FFR value. For the comparable task of predictingrevascularisation decision, we are able to improve the performance in terms ofAUC of both existing approaches with the proposed modifications, specificallyfrom 0.80 to 0.90 for the 3D-RCNN, and from 0.85 to 0.90 for the multi-viewtexture-based ensemble. The newly proposed 2.5D approach achieves comparableresults with an AUC of 0.90."," Felix Denzinger, Michael Wels, Katharina Breininger, Anika Reidelsh&#xf6;fer, Joachim Eckert, Michael S&#xfc;hling, Axel Schmermund, Andreas Maier",http://arxiv.org/abs/1912.06417
95,Multi-level Similarity Learning for Low-Shot Recognition.,"Low-shot learning indicates the ability to recognize unseen objects based onvery limited labeled training samples, which simulates human visualintelligence. According to this concept, we propose a multi-level similaritymodel (MLSM) to capture the deep encoded distance metric between the supportand query samples. Our approach is achieved based on the fact that the imagesimilarity learning can be decomposed into image-level, global-level, andobject-level. Once the similarity function is established, MLSM will be able toclassify images for unseen classes by computing the similarity scores between alimited number of labeled samples and the target images. Furthermore, weconduct 5-way experiments with both 1-shot and 5-shot setting on Caltech-UCSDdatasets. It is demonstrated that the proposed model can achieve promisingresults compared with the existing methods in practical applications."," Hongwei Xv, Xin Sun, Junyu Dong, Shu Zhang, Qiong Li",http://arxiv.org/abs/1912.06418
96,Reducing Inefficiency in Carbon Auctions with Imperfect Competition.,"We study auctions for carbon licenses, a policy tool used to control thesocial cost of pollution. Each identical license grants the right to produce aunit of pollution. Each buyer (i.e., firm that pollutes during themanufacturing process) enjoys a decreasing marginal value for licenses, butsociety suffers an increasing marginal cost for each license distributed. Theseller (i.e., the government) can choose a number of licenses to put up forauction, and wishes to maximize the societal welfare: the total economic valueof the buyers minus the social cost. Motivated by emission license marketsdeployed in practice, we focus on uniform price auctions with a price floorand/or price ceiling. The seller has distributional information about themarket, and their goal is to tune the auction parameters to maximize expectedwelfare. The target benchmark is the maximum expected welfare achievable by anysuch auction under truth-telling behavior. Unfortunately, the uniform priceauction is not truthful, and strategic behavior can significantly reduce (evenbelow zero) the welfare of a given auction configuration.</p><p>We describe a subclass of ""safe-price&apos;"" auctions for which the welfare at anyBayes-Nash equilibrium will approximate the welfare under truth-tellingbehavior. We then show that the better of a safe-price auction, or a truthfulauction that allocates licenses to only a single buyer, will approximate thetarget benchmark. In particular, we show how to choose a number of licenses anda price floor so that the worst-case welfare, at any equilibrium, is a constantapproximation to the best achievable welfare under truth-telling afterexcluding the welfare contribution of a single buyer."," Kira Goldner, Nicole Immorlica, Brendan Lucier",http://arxiv.org/abs/1912.06428
97,End-to-End Learning of Visual Representations from Uncurated Instructional Videos.,"Annotating videos is cumbersome, expensive and not scalable. Yet, many strongvideo models still rely on manually annotated data. With the recentintroduction of the HowTo100M dataset, narrated videos now offer thepossibility of learning video representations without manual supervision. Inthis work we propose a new learning approach, MIL-NCE, capable of addressingmisalignments inherent to narrated videos. With this approach we are able tolearn strong video representations from scratch, without the need for anymanual annotation. We evaluate our representations on a wide range of fourdownstream tasks over eight datasets: action recognition (HMDB-51, UCF-101,Kinetics-700), text-to-video retrieval (YouCook2, MSR-VTT), action localization(YouTube-8M Segments, CrossTask) and action segmentation (COIN). Our methodoutperforms all published self-supervised approaches for these tasks as well asseveral fully supervised baselines."," Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, Andrew Zisserman",http://arxiv.org/abs/1912.06430
98,A Bayesian Approach to Rule Mining.,"In this paper, we introduce the increasing belief criterion in associationrule mining. The criterion uses a recursive application of Bayes&apos; theorem tocompute a rule&apos;s belief. Extracted rules are required to have their beliefincrease with their last observation. We extend the taxonomy of associationrule mining algorithms with a new branch for Bayesian rule mining~(BRM), whichuses increasing belief as the rule selection criterion. In contrast, thewell-established frequent association rule mining~(FRM) branch relies on theminimum-support concept to extract rules.</p><p>We derive properties of the increasing belief criterion, such as theincreasing belief boundary, no-prior-worries, and conjunctive premises.Subsequently, we implement a BRM algorithm using the increasing beliefcriterion, and illustrate its functionality in three experiments: (1)~aproof-of-concept to illustrate BRM properties, (2)~an analysis relatingsocioeconomic information and chemical exposure data, and (3)~mining behaviourroutines in patients undergoing neurological rehabilitation. We illustrate howBRM is capable of extracting rare rules and does not suffer from supportdilution. Furthermore, we show that BRM focuses on the individual eventgenerating processes, while FRM focuses on their commonalities. We considerBRM&apos;s increasing belief as an alternative criterion to thresholds on rulesupport, as often applied in FRM, to determine rule usefulness."," Luis Ignacio Lopera Gonz&#xe1;lez, Adrian Derungs, Oliver Amft (Friedrich-Alexander University Erlangen-N&#xfc;rnberg, Erlangen, Germany)",http://arxiv.org/abs/1912.06432
99,Learning to Observe: Approximating Human Perceptual Thresholds for Detection of Suprathreshold Image Transformations.,"Many tasks in computer vision are often calibrated and evaluated relative tohuman perception. In this paper, we propose to directly approximate theperceptual function performed by human observers completing a visual detectiontask. Specifically, we present a novel methodology for learning to detect imagetransformations visible to human observers through approximating perceptualthresholds. To do this, we carry out a subjective two-alternative forced-choicestudy to estimate perceptual thresholds of human observers detecting localexposure shifts in images. We then leverage transformation equivariantrepresentation learning to overcome issues of limited perceptual data. Thisrepresentation is then used to train a dense convolutional classifier capableof detecting local suprathreshold exposure shifts - a distortion common toimage composites. In this context, our model is able to approximate perceptualthresholds with an average error of 0.1148 exposure stops between empirical andpredicted thresholds. It can also be trained to detect a range of differentpixel-wise transformation."," Alan Dolhasz, Carlo Harvey, Ian Williams",http://arxiv.org/abs/1912.06433
